modules:
- name: setup
  filename: setup.py
  metadata: {}
  components: []
  dependencies: []
- name: cli\main
  filename: cli\main.py
  metadata: {}
  components:
  - functions:
    - name: setup_logging
      type: function
      parameters:
      - log_level
      docstring: Setup logging configuration.
      metadata: {}
      dependencies:
      - ValueError
      - getattr
      - isinstance
      - log_level.upper
      - logging.FileHandler
      - logging.StreamHandler
      - logging.basicConfig
      - os.makedirs
      line: 14
    - name: cli
      type: function
      parameters:
      - ctx
      - config
      - log_level
      docstring: Command line interface for AI Agent System.
      metadata: {}
      dependencies:
      - click.group
      - click.option
      - create_default_config
      - ctx.ensure_object
      - setup_logging
      line: 38
    - name: agent
      type: function
      parameters: []
      docstring: Manage AI agents.
      metadata: {}
      dependencies:
      - cli.group
      line: 51
    - name: list_agents
      type: function
      parameters:
      - ctx
      docstring: List all configured agents and their status.
      metadata: {}
      dependencies:
      - agent.command
      - agent_config.get
      - click.echo
      - config.get
      - items
      - load_config
      line: 57
    - name: start_agent
      type: function
      parameters:
      - ctx
      - agent_name
      docstring: Start a specific agent.
      metadata: {}
      dependencies:
      - _start_agent
      - agent.command
      - asyncio.run
      - click.argument
      line: 71
    - name: stop_agent
      type: function
      parameters:
      - ctx
      - agent_name
      docstring: Stop a specific agent.
      metadata: {}
      dependencies:
      - _stop_agent
      - agent.command
      - asyncio.run
      - click.argument
      line: 109
    - name: agent_status
      type: function
      parameters:
      - ctx
      - agent_name
      docstring: Check the status of agents.
      metadata: {}
      dependencies:
      - _agent_status
      - agent.command
      - asyncio.run
      - click.argument
      line: 131
    - name: config
      type: function
      parameters: []
      docstring: Manage system configuration.
      metadata: {}
      dependencies:
      - cli.group
      line: 162
    - name: show_config
      type: function
      parameters:
      - ctx
      docstring: Show the current configuration.
      metadata: {}
      dependencies:
      - agent_config.get
      - click.echo
      - config.command
      - config.get
      - get
      - items
      - load_config
      - model.get
      - provider_config.get
      line: 168
    - name: logs
      type: function
      parameters: []
      docstring: View and manage logs.
      metadata: {}
      dependencies:
      - cli.group
      line: 201
    - name: view_logs
      type: function
      parameters:
      - ctx
      - agent_name
      - lines
      docstring: View logs for the system or a specific agent.
      metadata: {}
      dependencies:
      - click.argument
      - click.echo
      - click.option
      - config.get
      - file.readlines
      - get
      - len
      - line.strip
      - load_config
      - logs.command
      - open
      - os.path.exists
      - os.path.join
      - str
      line: 209
    filename: C:/Users/mad_p/OneDrive/Desktop/Py Projects/arxiv_pipeline/cli/main.py
  dependencies: []
- name: cli\utils
  filename: cli\utils.py
  metadata: {}
  components: []
  dependencies: []
- name: cli\commands\agent
  filename: cli\commands\agent.py
  metadata: {}
  components: []
  dependencies: []
- name: cli\commands\config
  filename: cli\commands\config.py
  metadata: {}
  components: []
  dependencies: []
- name: cli\commands\logs
  filename: cli\commands\logs.py
  metadata: {}
  components: []
  dependencies: []
- name: config\settings
  filename: config\settings.py
  metadata: {}
  components: []
  dependencies: []
- name: dev_utils\metadata_generator
  filename: dev_utils\metadata_generator.py
  metadata: {}
  components:
  - functions:
    - name: find_project_root
      type: function
      parameters: []
      docstring: Find the project root directory by looking for the .git folder.
      metadata: {}
      dependencies:
      - Path
      - Path.cwd
      - exists
      - resolve
      line: 29
    - name: parse_metadata
      type: function
      parameters:
      - docstring
      docstring: Extract YAML metadata from docstring
      metadata: {}
      dependencies:
      - match.group
      - re.search
      - yaml.safe_load
      line: 40
    - name: parse_python_file
      type: function
      parameters:
      - file_path
      docstring: Parse a Python file and extract metadata.
      metadata: {}
      dependencies:
      - CodeAnalyzer
      - Path
      - analyzer.modules.values
      - analyzer.visit
      - ast.parse
      - f.read
      - first_comment.group
      - open
      - parse_metadata
      - print
      - re.search
      - rel_path.replace
      - relative_to
      - resolve
      - str
      line: 133
    - name: generate_system_metadata
      type: function
      parameters:
      - project_root
      - output_file
      docstring: ''
      metadata: {}
      dependencies:
      - Path
      - append
      - component.get
      - extend
      - file.endswith
      - file_data.get
      - list
      - open
      - os.path.join
      - os.walk
      - parse_python_file
      - print
      - relative_file_path.replace
      - relative_to
      - replace
      - set
      - sorted
      - str
      - yaml.dump
      line: 170
    classes:
    - name: CodeAnalyzer
      type: class
      methods:
      - name: __init__
        type: method
        parameters:
        - self
        docstring: ''
        metadata: {}
        dependencies: []
        line: 57
      - name: visit_Module
        type: method
        parameters:
        - self
        - node
        docstring: ''
        metadata: {}
        dependencies:
        - self.generic_visit
        line: 62
      - name: visit_ClassDef
        type: method
        parameters:
        - self
        - node
        docstring: ''
        metadata: {}
        dependencies:
        - append
        - ast.get_docstring
        - docstring.split
        - parse_metadata
        - self._get_dependencies
        - self.generic_visit
        - self.modules.setdefault
        - setdefault
        - strip
        line: 67
      - name: visit_FunctionDef
        type: method
        parameters:
        - self
        - node
        docstring: ''
        metadata: {}
        dependencies:
        - append
        - ast.get_docstring
        - docstring.split
        - parse_metadata
        - self._get_dependencies
        - self.generic_visit
        - self.modules.setdefault
        - setdefault
        - strip
        line: 85
      - name: _get_dependencies
        type: method
        parameters:
        - self
        - node
        docstring: ''
        metadata: {}
        dependencies:
        - _get_full_name
        - ast.walk
        - dependencies.add
        - dependencies.update
        - isinstance
        - set
        - sorted
        line: 106
      - name: _get_full_name
        type: method
        parameters:
        - node
        docstring: ''
        metadata: {}
        dependencies:
        - _get_full_name
        - isinstance
        line: 109
      docstring: ''
      metadata: {}
      dependencies:
      - _get_full_name
      - append
      - ast.get_docstring
      - ast.walk
      - dependencies.add
      - dependencies.update
      - docstring.split
      - isinstance
      - parse_metadata
      - self._get_dependencies
      - self.generic_visit
      - self.modules.setdefault
      - set
      - setdefault
      - sorted
      - strip
      line: 56
    filename: C:/Users/mad_p/OneDrive/Desktop/Py Projects/arxiv_pipeline/dev_utils/metadata_generator.py
  dependencies: []
- name: scripts\check_prometheus_metrics
  filename: scripts\check_prometheus_metrics.py
  metadata: {}
  components:
  - functions:
    - name: check_prometheus_up
      type: function
      parameters:
      - base_url
      docstring: Check if Prometheus is running and responding
      metadata: {}
      dependencies:
      - info.get
      - print
      - requests.get
      - response.json
      line: 17
    - name: get_metrics_list
      type: function
      parameters:
      - base_url
      docstring: Get a list of all available metrics in Prometheus
      metadata: {}
      dependencies:
      - print
      - requests.get
      - response.json
      line: 32
    - name: check_container_metrics
      type: function
      parameters:
      - base_url
      docstring: Check if container metrics are available
      metadata: {}
      dependencies:
      - available.append
      - len
      - print
      - requests.get
      - response.json
      line: 46
    - name: check_host_metrics
      type: function
      parameters:
      - base_url
      docstring: Check if host metrics are available
      metadata: {}
      dependencies:
      - available.append
      - len
      - print
      - requests.get
      - response.json
      line: 70
    - name: check_targets
      type: function
      parameters:
      - base_url
      docstring: Check Prometheus targets and their status
      metadata: {}
      dependencies:
      - get
      - len
      - print
      - requests.get
      - response.json
      - sum
      - target.get
      line: 94
    - name: check_container_labels
      type: function
      parameters:
      - base_url
      docstring: Check what labels are available for container metrics
      metadata: {}
      dependencies:
      - all_labels.update
      - get
      - join
      - keys
      - len
      - list
      - print
      - requests.get
      - response.json
      - set
      - sorted
      line: 116
    - name: check_mongodb_metrics
      type: function
      parameters:
      - base_url
      docstring: Check for MongoDB metrics critical for ArXiv pipeline vector database
        operations
      metadata: {}
      dependencies:
      - available.append
      - connection_states.append
      - join
      - len
      - operation_types.append
      - print
      - requests.get
      - response.json
      - set
      line: 161
    - name: verify_dashboard_queries
      type: function
      parameters:
      - base_url
      docstring: Verify that key queries used in the ArXiv Pipeline dashboard are
        working
      metadata: {}
      dependencies:
      - len
      - print
      - requests.get
      - response.json
      line: 205
    - name: main
      type: function
      parameters: []
      docstring: ''
      metadata: {}
      dependencies:
      - argparse.ArgumentParser
      - bool
      - check_container_labels
      - check_container_metrics
      - check_host_metrics
      - check_mongodb_metrics
      - check_prometheus_up
      - check_targets
      - datetime.now
      - get_metrics_list
      - len
      - parser.add_argument
      - parser.parse_args
      - print
      - strftime
      - sys.exit
      - verify_dashboard_queries
      line: 230
    filename: C:/Users/mad_p/OneDrive/Desktop/Py Projects/arxiv_pipeline/scripts/check_prometheus_metrics.py
  dependencies: []
- name: src\__init__
  filename: src\__init__.py
  metadata: {}
  components: []
  dependencies: []
- name: src\agents\base_agent
  filename: src\agents\base_agent.py
  metadata: {}
  components:
  - classes:
    - name: BaseAgent
      type: class
      methods:
      - name: __init__
        type: method
        parameters:
        - self
        - name
        - config
        - model_interface
        docstring: ''
        metadata: {}
        dependencies:
        - logging.getLogger
        line: 11
      - name: _get_interval_seconds
        type: method
        parameters:
        - self
        docstring: Get the interval in seconds based on configuration.
        metadata: {}
        dependencies:
        - self.config.get
        line: 64
      docstring: Base class for all agents.
      metadata: {}
      dependencies:
      - asyncio.sleep
      - datetime.now
      - logging.getLogger
      - self._get_interval_seconds
      - self.config.get
      - self.initialize
      - self.logger.error
      - self.logger.info
      - self.logger.warning
      - self.run_cycle
      - str
      line: 8
    filename: C:/Users/mad_p/OneDrive/Desktop/Py Projects/arxiv_pipeline/src/agents/base_agent.py
  dependencies: []
- name: src\agents\manager
  filename: src\agents\manager.py
  metadata: {}
  components:
  - classes:
    - name: AgentManager
      type: class
      methods:
      - name: __init__
        type: method
        parameters:
        - self
        - config_path
        docstring: ''
        metadata: {}
        dependencies:
        - load_config
        - logging.getLogger
        line: 16
      - name: _get_model_provider
        type: method
        parameters:
        - self
        - model_name
        docstring: Determine which provider owns a specific model.
        metadata: {}
        dependencies:
        - items
        - model.get
        - model_config.get
        - provider_config.get
        - self.config.get
        line: 92
      - name: get_agent_status
        type: method
        parameters:
        - self
        - agent_name
        docstring: Get the status of a specific agent.
        metadata: {}
        dependencies:
        - agent.last_run.isoformat
        line: 150
      - name: list_agents
        type: method
        parameters:
        - self
        docstring: List all agents and their status.
        metadata: {}
        dependencies:
        - self.get_agent_status
        line: 163
      docstring: Manages the lifecycle of all agents in the system.
      metadata: {}
      dependencies:
      - agent.last_run.isoformat
      - agent.start
      - agent.stop
      - agent_class
      - agent_config.get
      - agent_configs.items
      - agent_name.title
      - asyncio.create_task
      - getattr
      - importlib.import_module
      - items
      - load_config
      - logging.getLogger
      - model.get
      - model_class
      - model_config.get
      - model_interface.initialize
      - provider_config.get
      - provider_name.capitalize
      - replace
      - self._get_model_provider
      - self._initialize_agents
      - self._initialize_models
      - self.agents.items
      - self.config.get
      - self.get_agent_status
      - self.logger.error
      - self.logger.info
      - self.tasks.append
      - str
      line: 13
    filename: C:/Users/mad_p/OneDrive/Desktop/Py Projects/arxiv_pipeline/src/agents/manager.py
  dependencies: []
- name: src\agents\code_doc\agent
  filename: src\agents\code_doc\agent.py
  metadata: {}
  components:
  - classes:
    - name: CodeDocAgent
      type: class
      methods:
      - name: __init__
        type: method
        parameters:
        - self
        - config
        - model_interface
        docstring: ''
        metadata: {}
        dependencies:
        - CodeParser
        - __init__
        - config.get
        - super
        line: 18
      - name: _filter_relevant_changes
        type: method
        parameters:
        - self
        - changes
        docstring: Filter changes based on ignore patterns.
        metadata: {}
        dependencies:
        - any
        - change.get
        - filtered.append
        - re.match
        line: 85
      - name: _create_doc_prompt
        type: method
        parameters:
        - self
        - parsed_code
        - change
        docstring: Create a prompt for documentation generation.
        metadata: {}
        dependencies:
        - change.get
        - parsed_code.get
        line: 124
      - name: _format_results
        type: method
        parameters:
        - self
        - results
        docstring: Format results for storage or display.
        metadata: {}
        dependencies:
        - result.get
        line: 152
      docstring: Agent for monitoring code changes and suggesting documentation updates.
      metadata: {}
      dependencies:
      - CodeParser
      - GitMonitor
      - Path
      - __init__
      - any
      - change.get
      - config.get
      - filtered.append
      - len
      - os.path.exists
      - parsed_code.get
      - re.match
      - read_text
      - result.get
      - results.append
      - self._analyze_file_changes
      - self._analyze_git_changes
      - self._create_doc_prompt
      - self._filter_relevant_changes
      - self._format_results
      - self._process_changes
      - self.config.get
      - self.git_monitor.get_recent_changes
      - self.logger.error
      - self.logger.info
      - self.model.generate
      - self.parser.parse
      - str
      - super
      line: 15
    filename: C:/Users/mad_p/OneDrive/Desktop/Py Projects/arxiv_pipeline/src/agents/code_doc/agent.py
  dependencies: []
- name: src\agents\code_doc\code_parser
  filename: src\agents\code_doc\code_parser.py
  metadata: {}
  components: []
  dependencies: []
- name: src\agents\code_doc\git_monitor
  filename: src\agents\code_doc\git_monitor.py
  metadata: {}
  components: []
  dependencies: []
- name: src\agents\research\agent
  filename: src\agents\research\agent.py
  metadata: {}
  components:
  - classes:
    - name: ResearchAnalysisAgent
      type: class
      methods:
      - name: __init__
        type: method
        parameters:
        - self
        - config
        - model_interface
        docstring: ''
        metadata: {}
        dependencies:
        - ConceptMapper
        - PaperProcessor
        - __init__
        - config.get
        - super
        line: 18
      - name: _get_default_prompt
        type: method
        parameters:
        - self
        - task_name
        docstring: Get a default prompt template for a task.
        metadata: {}
        dependencies: []
        line: 162
      docstring: Agent for analyzing research papers and generating insights.
      metadata: {}
      dependencies:
      - ConceptMapper
      - MongoDBClient
      - Neo4jClient
      - PaperProcessor
      - QdrantClient
      - __init__
      - all_papers.extend
      - client.get_collection
      - client.run_query
      - collection.find
      - config.get
      - f.read
      - join
      - len
      - list
      - open
      - os.environ.get
      - os.path.exists
      - paper.get
      - prompt_template.format
      - replace
      - self._fetch_papers
      - self._get_default_prompt
      - self._process_task
      - self.logger.info
      - self.model.generate
      - self.mongodb_clients.items
      - self.neo4j_clients.items
      - self.vector_store_config.get
      - source.get
      - startswith
      - str
      - super
      - task.get
      line: 15
    filename: C:/Users/mad_p/OneDrive/Desktop/Py Projects/arxiv_pipeline/src/agents/research/agent.py
  dependencies: []
- name: src\agents\research\concept_mapper
  filename: src\agents\research\concept_mapper.py
  metadata: {}
  components: []
  dependencies: []
- name: src\agents\research\paper_processor
  filename: src\agents\research\paper_processor.py
  metadata: {}
  components: []
  dependencies: []
- name: src\agents_core\config
  filename: src\agents_core\config.py
  metadata: {}
  components:
  - functions:
    - name: load_config
      type: function
      parameters:
      - config_path
      docstring: 'Load and validate configuration from a YAML file.

        Replaces environment variables in the format ${ENV_VAR}.'
      metadata: {}
      dependencies:
      - FileNotFoundError
      - ValueError
      - _replace_env_vars
      - _validate_config
      - logger.error
      - logging.getLogger
      - open
      - os.path.exists
      - str
      - yaml.safe_load
      line: 13
    - name: _replace_env_vars
      type: function
      parameters:
      - obj
      docstring: Recursively replace environment variables in strings.
      metadata: {}
      dependencies:
      - _replace_env_vars
      - isinstance
      - obj.items
      - obj.replace
      - os.environ.get
      - re.findall
      line: 42
    - name: _validate_config
      type: function
      parameters:
      - config
      docstring: Validate the configuration structure.
      metadata: {}
      dependencies:
      - ValueError
      - agents_config.items
      - config.get
      - isinstance
      - logger.warning
      - logging.getLogger
      - models_config.get
      line: 59
    - name: create_default_config
      type: function
      parameters:
      - output_path
      docstring: Create a default configuration file if none exists.
      metadata: {}
      dependencies:
      - open
      - os.makedirs
      - os.path.dirname
      - os.path.exists
      - yaml.dump
      line: 106
    filename: C:/Users/mad_p/OneDrive/Desktop/Py Projects/arxiv_pipeline/src/agents_core/config.py
  dependencies: []
- name: src\agents_core\logging_utils
  filename: src\agents_core\logging_utils.py
  metadata: {}
  components:
  - functions:
    - name: setup_logger
      type: function
      parameters:
      - name
      - log_level
      docstring: "Configure a logger with the specified name and log level.\n\nArgs:\n\
        \    name: Logger name\n    log_level: Logging level (DEBUG, INFO, WARNING,\
        \ ERROR, CRITICAL)\n    \nReturns:\n    Configured logger instance"
      metadata: {}
      dependencies:
      - getattr
      - handler.setFormatter
      - log_level.upper
      - logger.addHandler
      - logger.setLevel
      - logging.Formatter
      - logging.StreamHandler
      - logging.getLogger
      line: 16
    - name: validate_paper_schema
      type: function
      parameters:
      - paper
      docstring: "Validate a paper document against the expected schema.\n\nArgs:\n\
        \    paper: Paper document dictionary\n    \nReturns:\n    Tuple of (is_valid,\
        \ list_of_errors)"
      metadata: {}
      dependencies:
      - all
      - datetime.strptime
      - errors.append
      - isinstance
      - len
      line: 45
    - name: validate_publication_date
      type: function
      parameters:
      - date_str
      docstring: "Validate a publication date string and convert to datetime object.\n\
        \nArgs:\n    date_str: ISO-format date string (YYYY-MM-DDThh:mm:ssZ)\n   \
        \ \nReturns:\n    Tuple of (is_valid, datetime_obj or None)"
      metadata: {}
      dependencies:
      - datetime
      - datetime.strptime
      - datetime.utcnow
      - isinstance
      - len
      line: 97
    - name: count_papers_by_date
      type: function
      parameters:
      - mongo_collection
      - date_field
      - group_by
      docstring: "Count papers by publication date with flexible grouping.\n\nArgs:\n\
        \    mongo_collection: MongoDB collection object\n    date_field: Field name\
        \ containing the date\n    group_by: Grouping level ('day', 'month', 'year',\
        \ 'weekday')\n    \nReturns:\n    OrderedDict of date counts"
      metadata: {}
      dependencies:
      - OrderedDict
      - ValueError
      - datetime.strptime
      - defaultdict
      - doc.get
      - dt.strftime
      - isinstance
      - list
      - mongo_collection.aggregate
      - mongo_collection.find
      - pipeline.append
      - pipeline.extend
      - weekday_counts.get
      line: 136
    - name: analyze_mongodb_collection
      type: function
      parameters:
      - mongo_collection
      - query
      - projection
      docstring: "Analyze the structure and content of a MongoDB collection.\n\nArgs:\n\
        \    mongo_collection: MongoDB collection object\n    query: Optional filter\
        \ query\n    projection: Optional field projection\n    \nReturns:\n    Dictionary\
        \ with analysis results"
      metadata: {}
      dependencies:
      - add
      - defaultdict
      - doc.items
      - isinstance
      - items
      - keys
      - len
      - limit
      - list
      - min
      - mongo_collection.count_documents
      - mongo_collection.find
      - set
      - str
      - type
      line: 206
    - name: generate_date_distribution_report
      type: function
      parameters:
      - date_counts
      - title
      docstring: "Generate a formatted report of date-based paper distribution.\n\n\
        Args:\n    date_counts: OrderedDict of dates and counts\n    title: Report\
        \ title\n    \nReturns:\n    Formatted report string"
      metadata: {}
      dependencies:
      - date_counts.items
      - date_counts.keys
      - date_counts.values
      - int
      - iter
      - join
      - len
      - lines.append
      - max
      - next
      - sum
      line: 279
    - name: validate_mongodb_data
      type: function
      parameters:
      - mongo_collection
      - validation_func
      - sample_size
      docstring: "Validate a sample of documents in a MongoDB collection.\n\nArgs:\n\
        \    mongo_collection: MongoDB collection object\n    validation_func: Function\
        \ that validates a document and returns (bool, [errors])\n    sample_size:\
        \ Number of documents to validate\n    \nReturns:\n    Dictionary with validation\
        \ results"
      metadata: {}
      dependencies:
      - defaultdict
      - dict
      - doc.get
      - len
      - list
      - min
      - mongo_collection.aggregate
      - mongo_collection.count_documents
      - sample_errors.append
      - str
      - validation_func
      line: 333
    - name: check_data_integrity
      type: function
      parameters:
      - mongo_collection
      - date_range
      docstring: "Check data integrity with focus on temporal consistency.\n\nArgs:\n\
        \    mongo_collection: MongoDB collection to check\n    date_range: Optional\
        \ tuple of (start_date, end_date) strings\n    \nReturns:\n    Dictionary\
        \ with integrity check results"
      metadata: {}
      dependencies:
      - count_papers_by_date
      - current_month.replace
      - datetime.strptime
      - datetime.utcnow
      - expected_next.replace
      - gap_date.replace
      - gap_date.strftime
      - gap_months.append
      - gaps.append
      - len
      - list
      - mongo_collection.aggregate
      - mongo_collection.count_documents
      - monthly_counts.items
      - months.sort
      - query.copy
      - range
      - strftime
      line: 399
    filename: C:/Users/mad_p/OneDrive/Desktop/Py Projects/arxiv_pipeline/src/agents_core/logging_utils.py
  dependencies: []
- name: src\agents_core\data\mongodb
  filename: src\agents_core\data\mongodb.py
  metadata: {}
  components:
  - classes:
    - name: MongoDBClient
      type: class
      methods:
      - name: __init__
        type: method
        parameters:
        - self
        - connection_string
        - database
        docstring: ''
        metadata: {}
        dependencies:
        - pymongo.MongoClient
        line: 8
      - name: get_collection
        type: method
        parameters:
        - self
        - collection_name
        docstring: Get a MongoDB collection.
        metadata: {}
        dependencies: []
        line: 12
      - name: store_agent_result
        type: method
        parameters:
        - self
        - agent_name
        - result
        docstring: Store an agent result in the database.
        metadata: {}
        dependencies:
        - collection.insert_one
        - pymongo.datetime.datetime.utcnow
        - self.get_collection
        - str
        line: 16
      - name: get_agent_results
        type: method
        parameters:
        - self
        - agent_name
        - limit
        - skip
        docstring: Get results for a specific agent.
        metadata: {}
        dependencies:
        - collection.find
        - cursor.sort
        - limit
        - list
        - self.get_collection
        - skip
        line: 24
      docstring: Client for MongoDB operations.
      metadata: {}
      dependencies:
      - collection.find
      - collection.insert_one
      - cursor.sort
      - limit
      - list
      - pymongo.MongoClient
      - pymongo.datetime.datetime.utcnow
      - self.get_collection
      - skip
      - str
      line: 5
    filename: C:/Users/mad_p/OneDrive/Desktop/Py Projects/arxiv_pipeline/src/agents_core/data/mongodb.py
  dependencies: []
- name: src\agents_core\data\neo4j
  filename: src\agents_core\data\neo4j.py
  metadata: {}
  components: []
  dependencies: []
- name: src\agents_core\data\qdrant
  filename: src\agents_core\data\qdrant.py
  metadata: {}
  components: []
  dependencies: []
- name: src\agents_core\models\base
  filename: src\agents_core\models\base.py
  metadata: {}
  components: []
  dependencies: []
- name: src\agents_core\models\claude
  filename: src\agents_core\models\claude.py
  metadata: {}
  components: []
  dependencies: []
- name: src\agents_core\models\huggingface
  filename: src\agents_core\models\huggingface.py
  metadata: {}
  components: []
  dependencies: []
- name: src\agents_core\models\ollama
  filename: src\agents_core\models\ollama.py
  metadata: {}
  components:
  - classes:
    - name: OllamaModelInterface
      type: class
      methods:
      - name: __init__
        type: method
        parameters:
        - self
        docstring: ''
        metadata: {}
        dependencies:
        - logging.getLogger
        line: 12
      docstring: Interface for interacting with Ollama models.
      metadata: {}
      dependencies:
      - aiohttp.ClientSession
      - config.get
      - get
      - logging.getLogger
      - model_config.get
      - models_data.get
      - parameters.get
      - params.update
      - response.json
      - response.text
      - response_data.get
      - self.logger.error
      - self.logger.info
      - self.logger.warning
      - session.get
      - session.post
      - str
      line: 9
    filename: C:/Users/mad_p/OneDrive/Desktop/Py Projects/arxiv_pipeline/src/agents_core/models/ollama.py
  dependencies: []
- name: src\api\main
  filename: src\api\main.py
  metadata: {}
  components: []
  dependencies: []
- name: src\api\models\mongodb
  filename: src\api\models\mongodb.py
  metadata: {}
  components:
  - classes:
    - name: MongoDBStats
      type: class
      methods: []
      docstring: Response model for MongoDB paper statistics
      metadata: {}
      dependencies: []
      line: 4
    - name: MongoDBConnectionResponse
      type: class
      methods: []
      docstring: Response model for MongoDB connection test
      metadata: {}
      dependencies: []
      line: 11
    - name: CategoryCount
      type: class
      methods: []
      docstring: Model for category count data
      metadata: {}
      dependencies: []
      line: 17
    - name: PaperAnalysisResponse
      type: class
      methods: []
      docstring: Response model for paper analysis by time
      metadata: {}
      dependencies: []
      line: 22
    filename: C:/Users/mad_p/OneDrive/Desktop/Py Projects/arxiv_pipeline/src/api/models/mongodb.py
  dependencies: []
- name: src\api\models\neo4j
  filename: src\api\models\neo4j.py
  metadata: {}
  components:
  - classes:
    - name: Neo4jConnectionResponse
      type: class
      methods: []
      docstring: Response model for Neo4j connection test
      metadata: {}
      dependencies: []
      line: 4
    - name: Neo4jStats
      type: class
      methods: []
      docstring: Response model for Neo4j database statistics
      metadata: {}
      dependencies: []
      line: 10
    - name: Neo4jGraphResponse
      type: class
      methods: []
      docstring: Response model for Neo4j graph data
      metadata: {}
      dependencies: []
      line: 17
    filename: C:/Users/mad_p/OneDrive/Desktop/Py Projects/arxiv_pipeline/src/api/models/neo4j.py
  dependencies: []
- name: src\api\models\qdrant
  filename: src\api\models\qdrant.py
  metadata: {}
  components:
  - classes:
    - name: QdrantStats
      type: class
      methods: []
      docstring: Response model for Qdrant paper statistics
      metadata: {}
      dependencies: []
      line: 4
    - name: QdrantConnectionResponse
      type: class
      methods: []
      docstring: Response model for Qdrant connection test
      metadata: {}
      dependencies: []
      line: 11
    - name: SyncResponse
      type: class
      methods: []
      docstring: Response model for sync operations
      metadata: {}
      dependencies: []
      line: 18
    - name: SyncStatusResponse
      type: class
      methods: []
      docstring: Response model for sync status check
      metadata: {}
      dependencies: []
      line: 24
    filename: C:/Users/mad_p/OneDrive/Desktop/Py Projects/arxiv_pipeline/src/api/models/qdrant.py
  dependencies: []
- name: src\api\routes\mongodb
  filename: src\api\routes\mongodb.py
  metadata: {}
  components:
  - functions:
    - name: mongodb_paper_stats
      type: function
      parameters: []
      docstring: ''
      metadata: {}
      dependencies:
      - MongoClient
      - author_set.update
      - category_set.update
      - client.close
      - isinstance
      - len
      - logger.error
      - logging.info
      - mongo_uri.replace
      - os.getenv
      - os.path.exists
      - papers_collection.count_documents
      - papers_collection.find
      - router.get
      - set
      - str
      line: 20
    - name: test_mongodb_connection
      type: function
      parameters: []
      docstring: ''
      metadata: {}
      dependencies:
      - MongoClient
      - client.admin.command
      - client.close
      - client.list_database_names
      - logging.info
      - mongo_uri.replace
      - os.getenv
      - os.path.exists
      - router.get
      - str
      line: 57
    - name: get_papers_by_time
      type: function
      parameters:
      - start_date
      - end_date
      - year_filter
      - category
      docstring: "Returns analysis of papers by year, month, and day.\n\nArgs:\n \
        \   start_date: Optional start date filter (format: YYYY-MM-DD)\n    end_date:\
        \ Optional end date filter (format: YYYY-MM-DD)\n    year_filter: Optional\
        \ year to filter results (e.g., 2023)\n\nReturns:\n    Dictionary with yearly,\
        \ monthly, and daily paper counts"
      metadata: {}
      dependencies:
      - Query
      - analyze_papers_by_year_month_day
      - daily_data.items
      - logging.info
      - mongo_uri.replace
      - monthly_data.items
      - os.getenv
      - os.path.exists
      - router.get
      - str
      - yearly_data.items
      line: 83
    filename: C:/Users/mad_p/OneDrive/Desktop/Py Projects/arxiv_pipeline/src/api/routes/mongodb.py
  dependencies: []
- name: src\api\routes\neo4j
  filename: src\api\routes\neo4j.py
  metadata: {}
  components:
  - functions:
    - name: get_driver
      type: function
      parameters: []
      docstring: Get a Neo4j driver instance with proper error handling
      metadata: {}
      dependencies:
      - GraphDatabase.driver
      - logger.error
      - str
      line: 24
    - name: test_neo4j_connection
      type: function
      parameters: []
      docstring: Test connection to Neo4j database
      metadata: {}
      dependencies:
      - driver.close
      - driver.session
      - get_driver
      - result.single
      - router.get
      - session.run
      - str
      line: 38
    - name: neo4j_db_stats
      type: function
      parameters: []
      docstring: Get Neo4j database statistics (papers, authors, categories)
      metadata: {}
      dependencies:
      - author_result.peek
      - author_result.single
      - category_result.peek
      - category_result.single
      - driver.close
      - driver.session
      - get_driver
      - logger.error
      - paper_result.peek
      - paper_result.single
      - router.get
      - session.run
      - str
      line: 72
    - name: run_neo4j_query
      type: function
      parameters:
      - cypher_query
      docstring: Run a Cypher query and return the results in a format suitable for
        visualization
      metadata: {}
      dependencies:
      - Body
      - dict
      - driver.close
      - driver.session
      - edges.append
      - get_driver
      - hasattr
      - isinstance
      - len
      - list
      - logger.error
      - node_map.values
      - record.keys
      - router.post
      - session.run
      - str
      - value.get
      line: 102
    filename: C:/Users/mad_p/OneDrive/Desktop/Py Projects/arxiv_pipeline/src/api/routes/neo4j.py
  dependencies: []
- name: src\api\routes\qdrant
  filename: src\api\routes\qdrant.py
  metadata: {}
  components:
  - functions:
    - name: run_script_in_background
      type: function
      parameters:
      - script_path
      docstring: ''
      metadata: {}
      dependencies:
      - len
      - subprocess.Popen
      line: 67
    - name: test_qdrant_connection
      type: function
      parameters: []
      docstring: Tests connection to Qdrant server.
      metadata: {}
      dependencies:
      - logger.error
      - logger.info
      - logger.warning
      - requests.get
      - router.get
      - str
      line: 91
    - name: qdrant_paper_stats
      type: function
      parameters: []
      docstring: 'Returns stats for Qdrant collection: vector count (papers), vector
        dimensions, and collection count.'
      metadata: {}
      dependencies:
      - collection.get
      - collection_resp.json
      - collections_resp.json
      - isinstance
      - json.dumps
      - len
      - logger.error
      - logger.info
      - requests.get
      - router.get
      - str
      - vectors_data.items
      line: 142
    - name: qdrant_summary_stats
      type: function
      parameters: []
      docstring: 'Returns stats for Qdrant summary collection: vector count (papers),
        vector dimensions, and collection count.'
      metadata: {}
      dependencies:
      - collection_resp.json
      - logger.error
      - logger.info
      - requests.get
      - router.get
      - str
      - vectors_data.items
      line: 211
    - name: sync_summary_vectors
      type: function
      parameters:
      - background_tasks
      docstring: Starts a background task to sync paper summaries from MongoDB to
        Qdrant.
      metadata: {}
      dependencies:
      - HTTPException
      - os.path.dirname
      - os.path.exists
      - os.path.join
      - router.post
      - run_script_in_background
      line: 258
    - name: check_sync_status
      type: function
      parameters:
      - process_id
      docstring: Checks the status of a background sync process.
      metadata: {}
      dependencies:
      - HTTPException
      - process.communicate
      - process.poll
      - router.get
      line: 282
    filename: C:/Users/mad_p/OneDrive/Desktop/Py Projects/arxiv_pipeline/src/api/routes/qdrant.py
  dependencies: []
- name: src\embedding\embed
  filename: src\embedding\embed.py
  metadata: {}
  components: []
  dependencies: []
- name: src\graph\neo4j_sync
  filename: src\graph\neo4j_sync.py
  metadata: {}
  components:
  - classes:
    - name: Neo4jSync
      type: class
      methods:
      - name: __init__
        type: method
        parameters:
        - self
        - uri
        - user
        - password
        docstring: ''
        metadata: {}
        dependencies:
        - GraphDatabase.driver
        - logger.error
        - logger.info
        - self.driver.session
        - session.run
        line: 9
      - name: close
        type: method
        parameters:
        - self
        docstring: ''
        metadata: {}
        dependencies:
        - self.driver.close
        line: 20
      - name: clear_database
        type: method
        parameters:
        - self
        docstring: Clear all data from the Neo4j database
        metadata: {}
        dependencies:
        - logger.info
        - self.driver.session
        - session.run
        line: 24
      - name: sync_papers
        type: method
        parameters:
        - self
        - papers
        docstring: Legacy method for backward compatibility
        metadata: {}
        dependencies:
        - self.sync_papers_batch
        line: 30
      - name: sync_papers_batch
        type: method
        parameters:
        - self
        - papers
        - sync_timestamp
        docstring: "Sync a batch of papers to Neo4j using optimized batch operations\n\
          \nArgs:\n    papers: List of paper documents from MongoDB\n    sync_timestamp:\
          \ Optional timestamp to mark this sync operation\n    \nReturns:\n    Tuple\
          \ of (success_count, error_count)"
        metadata: {}
        dependencies:
        - enumerate
        - len
        - logger.error
        - logger.info
        - paper.get
        - self.driver.session
        - session.write_transaction
        - time.time
        line: 35
      - name: _create_paper
        type: method
        parameters:
        - tx
        - paper
        docstring: ''
        metadata: {}
        dependencies:
        - paper.get
        - tx.run
        line: 94
      - name: _create_author_and_relationship
        type: method
        parameters:
        - tx
        - paper_id
        - author_name
        docstring: ''
        metadata: {}
        dependencies:
        - tx.run
        line: 115
      - name: _create_authors_batch
        type: method
        parameters:
        - tx
        - authors_data
        docstring: "Create multiple authors and their relationships to papers in a\
          \ single transaction\n\nArgs:\n    authors_data: List of (paper_id, author_name)\
          \ tuples"
        metadata: {}
        dependencies:
        - tx.run
        line: 127
      - name: _create_category_and_relationship
        type: method
        parameters:
        - tx
        - paper_id
        - category
        docstring: ''
        metadata: {}
        dependencies:
        - tx.run
        line: 145
      - name: _create_categories_batch
        type: method
        parameters:
        - tx
        - categories_data
        docstring: "Create multiple categories and their relationships to papers in\
          \ a single transaction\n\nArgs:\n    categories_data: List of (paper_id,\
          \ category) tuples"
        metadata: {}
        dependencies:
        - tx.run
        line: 157
      - name: _mark_paper_synced
        type: method
        parameters:
        - tx
        - paper_id
        - timestamp
        docstring: "Mark a paper as synced with the given timestamp\n\nArgs:\n   \
          \ paper_id: ID of the paper\n    timestamp: ISO format timestamp string"
        metadata: {}
        dependencies:
        - tx.run
        line: 175
      docstring: ''
      metadata: {}
      dependencies:
      - GraphDatabase.driver
      - enumerate
      - len
      - logger.error
      - logger.info
      - paper.get
      - self.driver.close
      - self.driver.session
      - self.sync_papers_batch
      - session.run
      - session.write_transaction
      - time.time
      - tx.run
      line: 8
    filename: C:/Users/mad_p/OneDrive/Desktop/Py Projects/arxiv_pipeline/src/graph/neo4j_sync.py
  dependencies: []
- name: src\graph\sync_mongo_to_neo4j
  filename: src\graph\sync_mongo_to_neo4j.py
  metadata: {}
  components:
  - functions:
    - name: load_config
      type: function
      parameters:
      - path
      docstring: Load configuration from YAML file
      metadata: {}
      dependencies:
      - logger.error
      - open
      - yaml.safe_load
      line: 12
    - name: main
      type: function
      parameters: []
      docstring: ''
      metadata: {}
      dependencies:
      - MongoClient
      - Neo4jSync
      - datetime.now
      - db.papers.count_documents
      - db.papers.find
      - isoformat
      - len
      - limit
      - list
      - load_config
      - logger.error
      - logger.info
      - mongo_client.close
      - neo4j_sync.close
      - neo4j_sync.sync_papers_batch
      - progress.close
      - progress.update
      - range
      - skip
      - sort
      - time.time
      - tqdm
      line: 21
    filename: C:/Users/mad_p/OneDrive/Desktop/Py Projects/arxiv_pipeline/src/graph/sync_mongo_to_neo4j.py
  dependencies: []
- name: src\graph\test_neo4j
  filename: src\graph\test_neo4j.py
  metadata: {}
  components: []
  dependencies: []
- name: src\ingestion\fetch
  filename: src\ingestion\fetch.py
  metadata: {}
  components:
  - classes:
    - name: ArxivClient
      type: class
      methods:
      - name: __init__
        type: method
        parameters:
        - self
        - max_results
        - sort_by
        - sort_order
        docstring: ''
        metadata: {}
        dependencies: []
        line: 14
      - name: fetch_papers
        type: method
        parameters:
        - self
        - category
        - search_query
        - start
        docstring: "Fetch papers from arXiv API.\n\nArgs:\n    category: arXiv category\
          \ (default: cs.AI)\n    search_query: Optional search terms\n    start:\
          \ Starting index for pagination\n    \nReturns:\n    List of paper metadata\
          \ dictionaries"
        metadata: {}
        dependencies:
        - join
        - logger.error
        - logger.info
        - query_parts.append
        - requests.get
        - self._parse_response
        line: 22
      - name: _parse_response
        type: method
        parameters:
        - self
        - xml_data
        docstring: Parse arXiv API XML response into list of dictionaries.
        metadata: {}
        dependencies:
        - ET.fromstring
        - append
        - entry.findall
        - len
        - logger.info
        - replace
        - results.append
        - root.findall
        - self._get_text
        line: 67
      - name: _get_text
        type: method
        parameters:
        - element
        - xpath
        - namespaces
        - default
        docstring: Helper to extract text from XML element with proper error handling.
        metadata: {}
        dependencies:
        - element.find
        - text.strip
        line: 114
      docstring: Client for fetching papers from arXiv API using Atom XML format.
      metadata: {}
      dependencies:
      - ET.fromstring
      - append
      - element.find
      - entry.findall
      - join
      - len
      - logger.error
      - logger.info
      - query_parts.append
      - replace
      - requests.get
      - results.append
      - root.findall
      - self._get_text
      - self._parse_response
      - text.strip
      line: 9
    filename: C:/Users/mad_p/OneDrive/Desktop/Py Projects/arxiv_pipeline/src/ingestion/fetch.py
  dependencies: []
- name: src\llm_eval\evaluate_llm_models
  filename: src\llm_eval\evaluate_llm_models.py
  metadata: {}
  components:
  - functions:
    - name: main
      type: function
      parameters: []
      docstring: ''
      metadata: {}
      dependencies:
      - Path
      - compare_results
      - enumerate
      - evaluate_model
      - len
      - model_paths.append
      - print
      - results_dir.mkdir
      - str
      line: 6
    filename: C:/Users/mad_p/OneDrive/Desktop/Py Projects/arxiv_pipeline/src/llm_eval/evaluate_llm_models.py
  dependencies: []
- name: src\llm_eval\metadata_generator
  filename: src\llm_eval\metadata_generator.py
  metadata: {}
  components:
  - functions:
    - name: parse_metadata
      type: function
      parameters:
      - docstring
      docstring: Extract YAML metadata from docstring
      metadata: {}
      dependencies:
      - match.group
      - re.search
      - yaml.safe_load
      line: 12
    - name: parse_python_file
      type: function
      parameters:
      - file_path
      docstring: ''
      metadata: {}
      dependencies:
      - CodeAnalyzer
      - analyzer.visit
      - ast.parse
      - f.read
      - first_comment.group
      - open
      - parse_metadata
      - re.search
      line: 105
    - name: generate_system_metadata
      type: function
      parameters:
      - project_root
      - output_file
      docstring: ''
      metadata: {}
      dependencies:
      - Path
      - append
      - component.get
      - extend
      - file.endswith
      - file_data.get
      - get
      - list
      - open
      - os.path.join
      - os.walk
      - parse_python_file
      - print
      - relative_file_path.replace
      - relative_to
      - replace
      - set
      - sorted
      - str
      - yaml.dump
      line: 125
    classes:
    - name: CodeAnalyzer
      type: class
      methods:
      - name: __init__
        type: method
        parameters:
        - self
        docstring: ''
        metadata: {}
        dependencies: []
        line: 29
      - name: visit_Module
        type: method
        parameters:
        - self
        - node
        docstring: ''
        metadata: {}
        dependencies:
        - self.generic_visit
        line: 34
      - name: visit_ClassDef
        type: method
        parameters:
        - self
        - node
        docstring: ''
        metadata: {}
        dependencies:
        - append
        - ast.get_docstring
        - docstring.split
        - parse_metadata
        - self._get_dependencies
        - self.generic_visit
        - self.modules.setdefault
        - setdefault
        - strip
        line: 39
      - name: visit_FunctionDef
        type: method
        parameters:
        - self
        - node
        docstring: ''
        metadata: {}
        dependencies:
        - append
        - ast.get_docstring
        - docstring.split
        - parse_metadata
        - self._get_dependencies
        - self.generic_visit
        - self.modules.setdefault
        - setdefault
        - strip
        line: 57
      - name: _get_dependencies
        type: method
        parameters:
        - self
        - node
        docstring: ''
        metadata: {}
        dependencies:
        - _get_full_name
        - ast.walk
        - dependencies.add
        - dependencies.update
        - isinstance
        - set
        - sorted
        line: 78
      - name: _get_full_name
        type: method
        parameters:
        - node
        docstring: ''
        metadata: {}
        dependencies:
        - _get_full_name
        - isinstance
        line: 81
      docstring: ''
      metadata: {}
      dependencies:
      - _get_full_name
      - append
      - ast.get_docstring
      - ast.walk
      - dependencies.add
      - dependencies.update
      - docstring.split
      - isinstance
      - parse_metadata
      - self._get_dependencies
      - self.generic_visit
      - self.modules.setdefault
      - set
      - setdefault
      - sorted
      - strip
      line: 28
    filename: C:/Users/mad_p/OneDrive/Desktop/Py Projects/arxiv_pipeline/src/llm_eval/metadata_generator.py
  dependencies: []
- name: src\llm_eval\evaluation\compare_results
  filename: src\llm_eval\evaluation\compare_results.py
  metadata: {}
  components:
  - functions:
    - name: compare_results
      type: function
      parameters:
      - model1_path
      - model2_path
      - output_path
      docstring: ''
      metadata: {}
      dependencies:
      - f.write
      - json.load
      - open
      - print
      line: 3
    filename: C:/Users/mad_p/OneDrive/Desktop/Py Projects/arxiv_pipeline/src/llm_eval/evaluation/compare_results.py
  dependencies: []
- name: src\llm_eval\evaluation\evaluate
  filename: src\llm_eval\evaluation\evaluate.py
  metadata: {}
  components:
  - functions:
    - name: evaluate_model
      type: function
      parameters:
      - model_name
      - test_data_path
      - output_path
      docstring: ''
      metadata: {}
      dependencies:
      - Path
      - append
      - calculate_bleu
      - calculate_distinct_n
      - calculate_meteor
      - calculate_rouge
      - calculate_self_bleu
      - generated_texts.append
      - isinstance
      - json.dump
      - json.load
      - len
      - load_model
      - model.generate
      - open
      - output_path.parent.mkdir
      - print
      - to
      - tokenizer
      - tokenizer.decode
      line: 9
    filename: C:/Users/mad_p/OneDrive/Desktop/Py Projects/arxiv_pipeline/src/llm_eval/evaluation/evaluate.py
  dependencies: []
- name: src\llm_eval\evaluation\metrics
  filename: src\llm_eval\evaluation\metrics.py
  metadata: {}
  components:
  - functions:
    - name: calculate_bleu
      type: function
      parameters:
      - reference
      - generated
      - n_gram
      docstring: ''
      metadata: {}
      dependencies:
      - SmoothingFunction
      - generated.split
      - reference.split
      - sentence_bleu
      line: 22
    - name: calculate_bleu
      type: function
      parameters:
      - reference
      - generated
      - n_gram
      docstring: ''
      metadata: {}
      dependencies:
      - SmoothingFunction
      - generated.split
      - reference.split
      - sentence_bleu
      line: 32
    - name: calculate_meteor
      type: function
      parameters:
      - reference
      - generated
      docstring: ''
      metadata: {}
      dependencies:
      - generated.split
      - meteor_score
      - reference.split
      line: 37
    - name: calculate_rouge
      type: function
      parameters:
      - reference
      - generated
      docstring: ''
      metadata: {}
      dependencies:
      - rouge_scorer.RougeScorer
      - scorer.score
      line: 41
    - name: calculate_distinct_n
      type: function
      parameters:
      - generated_texts
      - n
      docstring: ''
      metadata: {}
      dependencies:
      - len
      - ngrams.extend
      - range
      - set
      - text.split
      - zip
      line: 46
    - name: calculate_self_bleu
      type: function
      parameters:
      - generated_texts
      - n
      docstring: ''
      metadata: {}
      dependencies:
      - enumerate
      - len
      - range
      - scores.append
      - sentence_bleu
      - split
      - sum
      - text.split
      line: 54
    filename: C:/Users/mad_p/OneDrive/Desktop/Py Projects/arxiv_pipeline/src/llm_eval/evaluation/metrics.py
  dependencies: []
- name: src\llm_eval\models\hf_model_filter
  filename: src\llm_eval\models\hf_model_filter.py
  metadata: {}
  components: []
  dependencies: []
- name: src\llm_eval\models\hf_model_list
  filename: src\llm_eval\models\hf_model_list.py
  metadata: {}
  components: []
  dependencies: []
- name: src\llm_eval\models\load_model
  filename: src\llm_eval\models\load_model.py
  metadata: {}
  components:
  - functions:
    - name: load_model
      type: function
      parameters:
      - model_name
      docstring: ''
      metadata:
        developer: Tom Hanks
        movie_reference: Forrest Gump
        fun_fact: Loves typewriters
      dependencies:
      - AutoModelForCausalLM.from_pretrained
      - AutoTokenizer.from_pretrained
      - model.to
      - torch.cuda.is_available
      line: 4
    filename: C:/Users/mad_p/OneDrive/Desktop/Py Projects/arxiv_pipeline/src/llm_eval/models/load_model.py
  dependencies: []
- name: src\llm_eval\models\models_to_compare
  filename: src\llm_eval\models\models_to_compare.py
  metadata: {}
  components: []
  dependencies: []
- name: src\llm_eval\utils\helpers
  filename: src\llm_eval\utils\helpers.py
  metadata: {}
  components: []
  dependencies: []
- name: src\pipeline\download_kaggle_arxiv
  filename: src\pipeline\download_kaggle_arxiv.py
  metadata: {}
  components:
  - classes:
    - name: ConfigError
      type: class
      methods: []
      docstring: Raised when there is an error in the configuration.
      metadata: {}
      dependencies: []
      line: 36
    functions:
    - name: load_config
      type: function
      parameters: []
      docstring: "Load configuration from default.yaml and kaggle.json.\n\nReturns:\n\
        \    Dict containing the merged configuration."
      metadata: {}
      dependencies:
      - ConfigError
      - Path
      - creds_path.exists
      - getattr
      - json.load
      - logger.error
      - logger.setLevel
      - open
      - str
      - update
      - upper
      - yaml.safe_load
      line: 40
    - name: setup_environment
      type: function
      parameters:
      - config
      docstring: "Set up environment variables and Kaggle credentials.\n\nArgs:\n\
        \    config: Configuration dictionary"
      metadata: {}
      dependencies:
      - Path
      - Path.home
      - creds_path.exists
      - dest_file.chmod
      - kaggle_dir.mkdir
      - load_dotenv
      - logger.error
      - logger.info
      - logger.warning
      - shutil
      - shutil.copy2
      - str
      line: 74
    - name: ensure_directory_exists
      type: function
      parameters:
      - directory
      docstring: "Ensure the download directory exists, create it if it doesn't.\n\
        \nArgs:\n    directory: Path to the directory\n    \nReturns:\n    Path: Path\
        \ object for the directory"
      metadata: {}
      dependencies:
      - Path
      - logger.error
      - logger.info
      - path.absolute
      - path.mkdir
      - str
      line: 115
    - name: download_dataset
      type: function
      parameters:
      - dataset_name
      - download_path
      - version
      docstring: "Download and extract the arXiv dataset from Kaggle.\n\nArgs:\n \
        \   dataset_name: Name of the Kaggle dataset (e.g., \"Cornell-University/arxiv\"\
        )\n    download_path: Directory where the dataset will be saved\n    version:\
        \ Dataset version (default: \"1\")\n    \nReturns:\n    Path: Path to the\
        \ downloaded dataset directory\n    \nRaises:\n    RuntimeError: If the dataset\
        \ download or extraction fails"
      metadata: {}
      dependencies:
      - FileNotFoundError
      - KaggleApi
      - RuntimeError
      - api.authenticate
      - api.dataset_download_files
      - dataset_name.split
      - download_path.glob
      - download_path.mkdir
      - list
      - logger.error
      - logger.info
      - str
      - zip_path.exists
      - zip_path.unlink
      - zip_ref.extractall
      - zipfile.ZipFile
      line: 134
    - name: main
      type: function
      parameters: []
      docstring: Main function to handle the download process.
      metadata: {}
      dependencies:
      - Path
      - argparse.ArgumentParser
      - download_dataset
      - ensure_directory_exists
      - get
      - load_config
      - logger.error
      - logger.info
      - parser.add_argument
      - parser.parse_args
      - setup_environment
      - str
      - sys.exit
      line: 201
    filename: C:/Users/mad_p/OneDrive/Desktop/Py Projects/arxiv_pipeline/src/pipeline/download_kaggle_arxiv.py
  dependencies: []
- name: src\pipeline\insert_bertopic_mongodb
  filename: src\pipeline\insert_bertopic_mongodb.py
  metadata: {}
  components:
  - functions:
    - name: load_config
      type: function
      parameters:
      - config_path
      docstring: "Load configuration settings from a YAML file.\n\nArgs:\n    config_path\
        \ (str): Path to the YAML configuration file.\n    \nReturns:\n    Dict[str,\
        \ Any]: Dictionary containing configuration settings.\n    \nRaises:\n   \
        \ FileNotFoundError: If the config file doesn't exist.\n    yaml.YAMLError:\
        \ If the YAML file is malformed."
      metadata: {}
      dependencies:
      - open
      - yaml.safe_load
      line: 20
    - name: is_docker
      type: function
      parameters: []
      docstring: "Detect if the current environment is running inside a Docker container.\n\
        \nThis function checks for the presence of Docker-specific information in\
        \ the\nLinux cgroup file. This is a reliable way to detect if we're running\
        \ in a\nDocker container on Linux systems.\n\nReturns:\n    bool: True if\
        \ running in Docker, False otherwise.\n\nNote:\n    Always returns False on\
        \ non-Linux systems where /proc/1/cgroup doesn't exist."
      metadata: {}
      dependencies:
      - f.read
      - open
      line: 36
    - name: get_mongo_uri
      type: function
      parameters:
      - config
      docstring: "Get the appropriate MongoDB URI based on environment and configuration.\n\
        \nThis function determines the correct MongoDB connection URI using the following\n\
        priority order:\n1. MONGO_URI environment variable if set\n2. Docker connection\
        \ string if running in Docker\n3. Local connection string if running locally\n\
        \nArgs:\n    config (Dict[str, Any]): Configuration dictionary containing\
        \ MongoDB settings\n        under the 'bertopic.mongo' section.\n    \nReturns:\n\
        \    str: MongoDB connection URI to use.\n    \nNote:\n    The function expects\
        \ the config to have the following structure:\n    bertopic:\n      mongo:\n\
        \        connection_string: \"mongodb://mongodb:27017/\"  # For Docker\n \
        \       connection_string_local: \"mongodb://localhost:27017/\"  # For local"
      metadata: {}
      dependencies:
      - is_docker
      - os.environ.get
      line: 55
    - name: build_mongo_query
      type: function
      parameters:
      - config
      docstring: "Build a MongoDB query based on configuration filters.\n\nConstructs\
        \ a MongoDB query dictionary that filters papers based on:\n1. Categories\
        \ - matches papers in any of the specified categories\n2. Date range - filters\
        \ papers within the specified date range\n\nArgs:\n    config (Dict[str, Any]):\
        \ Configuration dictionary containing filter settings\n        under the 'bertopic'\
        \ section.\n    \nReturns:\n    Dict: MongoDB query dictionary with the following\
        \ possible structure:\n        {\n            'categories': {'$in': ['cs.AI',\
        \ 'cs.LG', ...]},\n            'published': {\n                '$gte': '2023-01-01',\n\
        \                '$lte': '2025-05-20'\n            }\n        }\n\nNote:\n\
        \    - Category filter is only added if categories are specified in config\n\
        \    - Date filter is only added if date_filter.enabled is True and\n    \
        \  at least one of start_date or end_date is specified"
      metadata: {}
      dependencies: []
      line: 88
    - name: process_batch
      type: function
      parameters:
      - papers
      - topic_model
      - mongo_collection
      docstring: "Process a batch of papers and store topics in MongoDB.\n\nArgs:\n\
        \    papers: List of paper documents from MongoDB\n    topic_model: Trained\
        \ BERTopic model\n    mongo_collection: MongoDB collection to store results\n\
        \nReturns:\n    int: Number of papers successfully processed"
      metadata: {}
      dependencies:
      - UpdateOne
      - datetime.now
      - doc.get
      - enumerate
      - float
      - get
      - int
      - logger.error
      - logger.warning
      - mongo_collection.bulk_write
      - str
      - topic_docs.append
      - topic_info.iterrows
      - topic_model.get_topic_info
      - topic_model.transform
      - topics_dict.get
      - zip
      line: 132
    - name: process_data
      type: function
      parameters:
      - config
      docstring: "Main processing function for extracting topics from paper summaries.\n\
        \nThis function handles the complete pipeline for topic extraction:\n1. Connects\
        \ to MongoDB using environment-appropriate connection string\n2. Applies category\
        \ and date filters from config\n3. Initializes BERTopic model\n4. Processes\
        \ papers in batches:\n   - First batch is used to fit the model\n   - Subsequent\
        \ batches use the fitted model\n5. Stores results in a separate MongoDB collection\n\
        \nArgs:\n    config (Dict[str, Any]): Configuration dictionary containing\
        \ all settings:\n        - MongoDB connection details\n        - Batch processing\
        \ settings\n        - Category filters\n        - Date filters\n        -\
        \ Collection names\n\nRaises:\n    pymongo.errors.ConnectionError: If MongoDB\
        \ connection fails\n    Exception: For any other processing errors\n\nNote:\n\
        \    - Uses consistent sorting (_id) for reliable pagination\n    - Supports\
        \ both Docker and local environments\n    - Implements batch processing for\
        \ memory efficiency\n    - Provides progress tracking via tqdm\n    - Respects\
        \ max_papers limit if configured"
      metadata: {}
      dependencies:
      - BERTopic
      - MongoClient
      - build_mongo_query
      - doc.get
      - get_mongo_uri
      - len
      - limit
      - list
      - logger.error
      - logger.info
      - logger.warning
      - min
      - papers_collection.count_documents
      - papers_collection.find
      - process_batch
      - range
      - skip
      - sort
      - str
      - topic_model.fit
      - tqdm
      line: 191
    - name: main
      type: function
      parameters: []
      docstring: "Main entry point for the BERTopic extraction pipeline.\n\nThis function\
        \ handles:\n1. Command line argument parsing\n2. Configuration loading\n3.\
        \ Pipeline execution with timing\n4. Error handling and logging\n\nCommand\
        \ Line Arguments:\n    --config: Path to YAML configuration file (default:\
        \ config/default.yaml)\n\nThe function measures and logs the total execution\
        \ time of the pipeline.\nAny unhandled exceptions are caught, logged, and\
        \ result in a non-zero\nexit status.\n\nExample Usage:\n    python -m src.pipeline.insert_bertopic_mongodb\
        \ --config config/default.yaml"
      metadata: {}
      dependencies:
      - argparse.ArgumentParser
      - datetime.now
      - load_config
      - logger.error
      - logger.info
      - parser.add_argument
      - parser.parse_args
      - process_data
      - str
      line: 301
    filename: C:/Users/mad_p/OneDrive/Desktop/Py Projects/arxiv_pipeline/src/pipeline/insert_bertopic_mongodb.py
  dependencies: []
- name: src\pipeline\insert_top2vec_mongodb
  filename: src\pipeline\insert_top2vec_mongodb.py
  metadata: {}
  components:
  - functions:
    - name: preprocess_text
      type: function
      parameters:
      - text
      docstring: Preprocess text for better topic modeling
      metadata: {}
      dependencies:
      - len
      - re.sub
      - strip
      - text.lower
      - text.replace
      - text.split
      line: 38
    - name: classify_papers
      type: function
      parameters:
      - summaries
      - doc_ids
      docstring: "Classify research papers into topics using Top2Vec\n\nArgs:\n  \
        \  summaries: List of paper summaries\n    doc_ids: Optional document IDs\n\
        \    \nReturns:\n    Dictionary with model and results"
      metadata: {}
      dependencies:
      - Counter
      - Top2Vec
      - enumerate
      - len
      - list
      - model.get_documents_topics
      - model.get_topic
      - model.get_topic_sizes
      - np.mean
      - preprocess_text
      - print
      - range
      - report_data.append
      - round
      - sorted
      - summary.split
      - topic_counter.keys
      line: 78
    - name: get_topic_hierarchy
      type: function
      parameters:
      - model
      - num_topics
      docstring: "Get hierarchical topic representation to improve interpretability\n\
        \nArgs:\n    model: Trained Top2Vec model\n    num_topics: Number of topics\
        \ to reduce to (None for auto-determination)"
      metadata: {}
      dependencies:
      - enumerate
      - int
      - len
      - max
      - min
      - model.get_topic
      - model.get_topic_reduction
      - model.hierarchical_topic_reduction
      - np.sqrt
      - print
      line: 191
    - name: plot_topic_distribution
      type: function
      parameters:
      - report_data
      docstring: Plot topic distribution as a bar chart
      metadata: {}
      dependencies:
      - bar.get_height
      - bar.get_width
      - bar.get_x
      - join
      - len
      - plt.bar
      - plt.figure
      - plt.show
      - plt.text
      - plt.tight_layout
      - plt.title
      - plt.xlabel
      - plt.xticks
      - plt.ylabel
      - range
      - split
      line: 227
    - name: evaluate_topic_quality
      type: function
      parameters:
      - model
      - doc_topics
      - doc_probs
      docstring: Evaluate topic quality using various metrics
      metadata: {}
      dependencies:
      - coherence_scores.append
      - len
      - model.get_num_topics
      - model.get_topic
      - np.mean
      - print
      - range
      - sum
      line: 251
    - name: load_config
      type: function
      parameters:
      - config_path
      docstring: "Load configuration settings from a YAML file.\n\nArgs:\n    config_path\
        \ (str): Path to the YAML configuration file.\n    \nReturns:\n    Dict[str,\
        \ Any]: Dictionary containing configuration settings.\n    \nRaises:\n   \
        \ FileNotFoundError: If the config file doesn't exist.\n    yaml.YAMLError:\
        \ If the YAML file is malformed."
      metadata: {}
      dependencies:
      - open
      - yaml.safe_load
      line: 283
    - name: is_docker
      type: function
      parameters: []
      docstring: Detect if running inside a Docker container.
      metadata: {}
      dependencies:
      - f.read
      - open
      line: 299
    - name: get_mongo_uri
      type: function
      parameters:
      - config
      docstring: Get the appropriate MongoDB URI based on environment and configuration.
      metadata: {}
      dependencies:
      - is_docker
      - os.environ.get
      line: 307
    - name: build_mongo_query
      type: function
      parameters:
      - config
      docstring: Build a MongoDB query based on configuration filters.
      metadata: {}
      dependencies:
      - date_filter.get
      - get
      line: 317
    - name: process_batch
      type: function
      parameters:
      - papers
      - topic_model
      - mongo_collection
      docstring: "Process a batch of papers and store topics in MongoDB.\n\nArgs:\n\
        \    papers: List of paper documents from MongoDB\n    topic_model: Trained\
        \ Top2Vec model\n    mongo_collection: MongoDB collection to store results\n\
        \nReturns:\n    int: Number of papers successfully processed"
      metadata: {}
      dependencies:
      - UpdateOne
      - datetime.now
      - doc.get
      - enumerate
      - float
      - get
      - hasattr
      - int
      - isinstance
      - len
      - list
      - logger.error
      - logger.info
      - logger.warning
      - min
      - mongo_collection.bulk_write
      - scores.tolist
      - str
      - topic_model.get_document_topics
      - updates.append
      - words.tolist
      - zip
      line: 340
    - name: process_data
      type: function
      parameters:
      - config
      docstring: Main processing function for extracting topics from paper summaries.
      metadata: {}
      dependencies:
      - EnhancedTopicModeler
      - MongoClient
      - build_mongo_query
      - doc.get
      - get_mongo_uri
      - len
      - limit
      - list
      - logger.error
      - logger.info
      - logger.warning
      - min
      - papers_collection.count_documents
      - papers_collection.find
      - process_batch
      - range
      - skip
      - sort
      - str
      - topic_model.fit
      - tqdm
      line: 447
    - name: main
      type: function
      parameters: []
      docstring: Main entry point for the Top2Vec extraction pipeline.
      metadata: {}
      dependencies:
      - argparse.ArgumentParser
      - datetime.now
      - load_config
      - logger.error
      - logger.info
      - parser.add_argument
      - parser.parse_args
      - process_data
      - str
      line: 525
    filename: C:/Users/mad_p/OneDrive/Desktop/Py Projects/arxiv_pipeline/src/pipeline/insert_top2vec_mongodb.py
  dependencies: []
- name: src\pipeline\kaggle_sample_code
  filename: src\pipeline\kaggle_sample_code.py
  metadata: {}
  components: []
  dependencies: []
- name: src\pipeline\run_pipeline
  filename: src\pipeline\run_pipeline.py
  metadata: {}
  components:
  - functions:
    - name: load_config
      type: function
      parameters:
      - config_path
      docstring: Load configuration from YAML file.
      metadata: {}
      dependencies:
      - open
      - yaml.safe_load
      line: 17
    - name: filter_papers_by_date
      type: function
      parameters:
      - papers
      - start_date
      - end_date
      docstring: 'Filter papers by published date (ISO format: YYYY-MM-DD).'
      metadata: {}
      dependencies:
      - datetime.datetime
      - datetime.fromisoformat
      line: 22
    - name: run_ingestion_pipeline
      type: function
      parameters:
      - config
      docstring: Run the arXiv ingestion pipeline.
      metadata: {}
      dependencies:
      - ArxivClient
      - MongoStorage
      - arxiv_client.fetch_papers
      - filter_papers_by_date
      - get
      - len
      - logger.info
      - mongo_storage.store_papers
      - range
      - time
      - time.sleep
      line: 32
    - name: main
      type: function
      parameters: []
      docstring: Main entry point with command line argument parsing.
      metadata: {}
      dependencies:
      - argparse.ArgumentParser
      - datetime.now
      - load_config
      - logger.error
      - logger.info
      - parser.add_argument
      - parser.parse_args
      - range
      - run_ingestion_pipeline
      - str
      line: 104
    filename: C:/Users/mad_p/OneDrive/Desktop/Py Projects/arxiv_pipeline/src/pipeline/run_pipeline.py
  dependencies: []
- name: src\pipeline\sync_mongodb
  filename: src\pipeline\sync_mongodb.py
  metadata: {}
  components:
  - functions:
    - name: load_config
      type: function
      parameters:
      - config_path
      docstring: Load configuration from YAML file.
      metadata: {}
      dependencies:
      - open
      - yaml.safe_load
      line: 17
    - name: filter_papers_by_date
      type: function
      parameters:
      - papers
      - start_date
      - end_date
      docstring: 'Filter papers by published date (ISO format: YYYY-MM-DD).'
      metadata: {}
      dependencies:
      - datetime.datetime
      - datetime.fromisoformat
      line: 22
    - name: run_ingestion_pipeline
      type: function
      parameters:
      - config
      docstring: Run the arXiv ingestion pipeline.
      metadata: {}
      dependencies:
      - ArxivClient
      - MongoStorage
      - arxiv_client.fetch_papers
      - filter_papers_by_date
      - get
      - len
      - logger.info
      - mongo_storage.store_papers
      - os
      - os.environ.get
      - range
      - time
      - time.sleep
      line: 32
    - name: main
      type: function
      parameters: []
      docstring: Main entry point with command line argument parsing.
      metadata: {}
      dependencies:
      - argparse.ArgumentParser
      - datetime.now
      - load_config
      - logger.error
      - logger.info
      - parser.add_argument
      - parser.parse_args
      - range
      - run_ingestion_pipeline
      - str
      line: 111
    filename: C:/Users/mad_p/OneDrive/Desktop/Py Projects/arxiv_pipeline/src/pipeline/sync_mongodb.py
  dependencies: []
- name: src\pipeline\sync_qdrant
  filename: src\pipeline\sync_qdrant.py
  metadata: {}
  components:
  - functions:
    - name: load_config
      type: function
      parameters: []
      docstring: ''
      metadata: {}
      dependencies:
      - open
      - os.path.dirname
      - os.path.join
      - yaml.safe_load
      line: 19
    - name: calculate_file_hash
      type: function
      parameters:
      - file_path
      docstring: Calculate SHA-256 hash of a file for unique identification.
      metadata: {}
      dependencies:
      - f.read
      - hashlib.sha256
      - iter
      - open
      - sha256.hexdigest
      - sha256.update
      line: 72
    - name: is_pdf_processed
      type: function
      parameters:
      - file_path
      - category
      docstring: Check if a PDF has already been processed and stored in Qdrant.
      metadata: {}
      dependencies:
      - os.path.basename
      - tracking_collection.find_one
      line: 80
    - name: mark_pdf_as_processed
      type: function
      parameters:
      - file_path
      - category
      - chunk_count
      docstring: Mark a PDF as processed in the tracking database.
      metadata: {}
      dependencies:
      - calculate_file_hash
      - datetime.datetime.now
      - os.path.basename
      - tracking_collection.update_one
      line: 92
    - name: sync_qdrant_with_tracking
      type: function
      parameters: []
      docstring: Synchronize MongoDB tracking with actual Qdrant contents.
      metadata: {}
      dependencies:
      - QdrantClient
      - len
      - print
      - qdrant_client.get_collection
      - qdrant_client.get_collections
      - set
      - str
      - tracked_files.add
      - tracking_collection.find
      line: 118
    - name: extract_images_from_pdf
      type: function
      parameters:
      - pdf_path
      - output_dir
      docstring: Extract images from PDF and save to output_dir. Returns list of image
        file paths.
      metadata: {}
      dependencies:
      - doc.get_page_images
      - enumerate
      - fitz.Pixmap
      - fitz.open
      - image_paths.append
      - len
      - os.path.join
      - pix.save
      - range
      line: 157
    - name: process_pdf
      type: function
      parameters:
      - pdf_path
      - qdrant_url
      - qdrant_collection
      docstring: ''
      metadata: {}
      dependencies:
      - LLMChain
      - Ollama
      - PromptTemplate
      - PyPDFLoader
      - QdrantClient
      - RecursiveCharacterTextSplitter
      - SentenceTransformer
      - enumerate
      - extract_images_from_pdf
      - get
      - get_embeddings
      - hash
      - image_chain.run
      - image_descriptions.append
      - len
      - loader.load
      - model.encode
      - os.makedirs
      - os.path.dirname
      - os.path.join
      - os.path.splitext
      - print
      - qdrant.create_collection
      - qdrant.get_collection
      - qdrant.upsert
      - qdrant_client.QdrantClient
      - qdrant_client.http.models
      - rest.PointStruct
      - rest.VectorParams
      - sentence_transformers.SentenceTransformer
      - str
      - text_splitter.split_documents
      - tolist
      - torch
      - torch.cuda.device_count
      - torch.cuda.is_available
      line: 172
    - name: get_embeddings
      type: function
      parameters:
      - texts
      docstring: ''
      metadata: {}
      dependencies:
      - model.encode
      - tolist
      line: 216
    - name: process_all_categories
      type: function
      parameters: []
      docstring: Process PDFs from all specified categories in the config.
      metadata: {}
      dependencies:
      - category_results.append
      - f.endswith
      - is_pdf_processed
      - len
      - mark_pdf_as_processed
      - os.listdir
      - os.makedirs
      - os.path.exists
      - os.path.join
      - print
      - process_pdf
      - root_results.append
      - str
      - sync_qdrant_with_tracking
      - unprocessed_files.append
      line: 315
    filename: C:/Users/mad_p/OneDrive/Desktop/Py Projects/arxiv_pipeline/src/pipeline/sync_qdrant.py
  dependencies: []
- name: src\pipeline\sync_summary_vectors
  filename: src\pipeline\sync_summary_vectors.py
  metadata: {}
  components:
  - functions:
    - name: load_config
      type: function
      parameters: []
      docstring: ''
      metadata: {}
      dependencies:
      - open
      - os.path.dirname
      - os.path.join
      - yaml.safe_load
      line: 13
    - name: is_paper_processed
      type: function
      parameters:
      - paper_id
      - category
      docstring: Check if a paper has already been processed and stored in Qdrant.
      metadata: {}
      dependencies:
      - tracking_collection.find_one
      line: 89
    - name: mark_paper_as_processed
      type: function
      parameters:
      - paper_id
      - category
      - summary_length
      docstring: Mark a paper as processed in the tracking database.
      metadata: {}
      dependencies:
      - datetime.datetime.now
      - print
      - tracking_collection.update_one
      line: 102
    - name: sync_qdrant_with_tracking
      type: function
      parameters: []
      docstring: Synchronize MongoDB tracking with actual Qdrant contents.
      metadata: {}
      dependencies:
      - QdrantClient
      - bulk_operations.append
      - client.create_collection
      - client.get_collections
      - client.scroll
      - datetime.datetime.now
      - isinstance
      - len
      - list
      - paper.get
      - papers_collection.find_one
      - print
      - pymongo.UpdateOne
      - qdrant_paper_ids.add
      - record.payload.get
      - set
      - str
      - tracking_collection.bulk_write
      - tracking_collection.delete_many
      - tracking_collection.find
      line: 124
    - name: create_query_filter
      type: function
      parameters: []
      docstring: Create a MongoDB query filter based on configuration settings.
      metadata: {}
      dependencies: []
      line: 268
    - name: process_papers
      type: function
      parameters: []
      docstring: Process papers from MongoDB and store their summaries in Qdrant.
      metadata: {}
      dependencies:
      - HuggingFaceEmbeddings
      - QdrantClient
      - batch_categories.append
      - batch_paper_ids.append
      - batch_summary_lengths.append
      - bulk_tracking_operations.append
      - client.create_collection
      - client.delete_collection
      - client.get_collections
      - client.upsert
      - create_query_filter
      - datetime.datetime.now
      - documents.append
      - embeddings.embed_documents
      - enumerate
      - hash
      - ids.append
      - int
      - isinstance
      - len
      - limit
      - list
      - metadata_list.append
      - paper.get
      - papers_collection.count_documents
      - papers_collection.find
      - payloads.append
      - print
      - pymongo.UpdateOne
      - range
      - reset_qdrant_collection
      - set
      - sort
      - str
      - sync_qdrant_with_tracking
      - traceback
      - traceback.print_exc
      - tracking_collection.bulk_write
      - tracking_collection.find
      - vectors.append
      - zip
      line: 291
    - name: reset_qdrant_collection
      type: function
      parameters:
      - client
      docstring: ''
      metadata: {}
      dependencies:
      - client.create_collection
      - client.delete_collection
      - client.get_collections
      - print
      - str
      line: 307
    filename: C:/Users/mad_p/OneDrive/Desktop/Py Projects/arxiv_pipeline/src/pipeline/sync_summary_vectors.py
  dependencies: []
- name: src\pipeline\top2vec
  filename: src\pipeline\top2vec.py
  metadata: {}
  components:
  - classes:
    - name: EnhancedTopicModeler
      type: class
      methods:
      - name: __init__
        type: method
        parameters:
        - self
        - min_count
        - speed
        - workers
        - embedding_model
        docstring: "Initialize the EnhancedTopicModeler.\n\nArgs:\n    min_count:\
          \ Minimum count of words to be included\n    speed: Speed vs accuracy tradeoff\
          \ ('learn', 'deep-learn', 'fast-learn')\n    workers: Number of worker threads\n\
          \    embedding_model: Which embedding model to use"
        metadata: {}
        dependencies:
        - logging.getLogger
        line: 33
      - name: fit
        type: method
        parameters:
        - self
        - documents
        - ids
        docstring: "Fit the model on the input documents.\n\nArgs:\n    documents:\
          \ List of document texts (paper summaries)\n    ids: Optional document IDs"
        metadata: {}
        dependencies:
        - Top2Vec
        - ValueError
        - len
        - np.mean
        - self._clean_text
        - self.logger.error
        - self.logger.info
        - self.model.get_num_topics
        - self.model.get_topic_sizes
        - self.model.get_topics
        - str
        line: 58
      - name: _clean_text
        type: method
        parameters:
        - self
        - text
        docstring: Clean text for better topic modeling.
        metadata: {}
        dependencies:
        - isinstance
        - len
        - re.sub
        - strip
        - text.lower
        - text.split
        line: 121
      - name: get_enhanced_topics
        type: method
        parameters:
        - self
        - num_words
        docstring: "Get enhanced topics with more meaningful words.\n\nArgs:\n   \
          \ num_words: Number of words per topic\n    \nReturns:\n    Tuple of (topic_words,\
          \ word_scores, topic_scores)"
        metadata: {}
        dependencies:
        - np.mean
        - self.logger.error
        - self.logger.info
        - self.model.get_topics
        - str
        line: 144
      - name: get_document_topics
        type: method
        parameters:
        - self
        docstring: "Get topic assignments for all documents.\n\nReturns:\n    Tuple\
          \ of (document_topics, document_topic_probs)"
        metadata: {}
        dependencies:
        - doc_probs.append
        - doc_topics.append
        - float
        - hasattr
        - int
        - isinstance
        - len
        - self.logger.error
        - self.logger.info
        - self.logger.warning
        - self.model.get_documents_topics
        - self.model.get_num_topics
        - str
        - type
        - zip
        line: 173
      - name: get_hierarchical_topics
        type: method
        parameters:
        - self
        - num_topics
        docstring: "Create hierarchical topic representation.\n\nArgs:\n    num_topics:\
          \ Target number of topics or None to auto-determine\n    \nReturns:\n  \
          \  Dictionary with hierarchy information"
        metadata: {}
        dependencies:
        - int
        - len
        - max
        - min
        - np.sqrt
        - print
        - self.model.hierarchical_topic_reduction
        line: 251
      - name: print_topic_report
        type: method
        parameters:
        - self
        - min_probability
        docstring: "Print a comprehensive topic report similar to BERTopic.\n\nArgs:\n\
          \    min_probability: Minimum probability threshold\n    \nReturns:\n  \
          \  DataFrame with topic report"
        metadata: {}
        dependencies:
        - df.iterrows
        - len
        - np.mean
        - pd.DataFrame
        - print
        - range
        - report_data.append
        - round
        - self.get_document_topics
        - self.model.get_num_topics
        - sorted
        - sum
        - topic_avg_probs.get
        - topic_counts.get
        - topic_counts.keys
        - topic_counts.values
        - zip
        line: 280
      - name: visualize_topics
        type: method
        parameters:
        - self
        - figsize
        docstring: Visualize topics in a 2D space.
        metadata: {}
        dependencies:
        - enumerate
        - plt.annotate
        - plt.figure
        - plt.scatter
        - plt.show
        - plt.tight_layout
        - plt.title
        - plt.xlabel
        - plt.ylabel
        - print
        - umap_model.transform
        - zip
        line: 341
      - name: get_topic_word_matrix
        type: method
        parameters:
        - self
        - num_words
        docstring: Create a topic-word matrix for additional analysis.
        metadata: {}
        dependencies:
        - all_words.update
        - enumerate
        - len
        - list
        - np.zeros
        - set
        - sorted
        - zip
        line: 376
      - name: reassign_outliers
        type: method
        parameters:
        - self
        - min_probability
        docstring: "Try to reassign outlier documents to the closest topic.\n\nReturns:\n\
          \    New document-topic assignments"
        metadata: {}
        dependencies:
        - enumerate
        - new_assignments.append
        - new_probs.append
        - np.argmax
        - np.dot
        - np.linalg.norm
        - self.get_document_topics
        - similarities.append
        line: 396
      - name: topic_keywords_table
        type: method
        parameters:
        - self
        - num_words
        docstring: Print a nice table of topic keywords.
        metadata: {}
        dependencies:
        - len
        - pd.DataFrame
        - range
        line: 443
      - name: document_topic_matrix
        type: method
        parameters:
        - self
        - documents
        - num_topics
        docstring: "Create a document-topic matrix for the given documents.\n\nArgs:\n\
          \    documents: List of document texts to analyze\n    num_topics: Number\
          \ of topics to consider per document\n    \nReturns:\n    Document-topic\
          \ matrix and topic names"
        metadata: {}
        dependencies:
        - doc_topic_matrix.sum
        - enumerate
        - join
        - len
        - np.dot
        - np.linalg.norm
        - np.zeros
        - self.model.embed
        - self.model.get_num_topics
        line: 462
      docstring: 'Enhanced topic modeling for research papers using Top2Vec with better

        topic naming and evaluation metrics. Designed to work with ArXiv papers.


        This class provides:

        1. Enhanced preprocessing for scientific text

        2. Better topic naming using domain-specific heuristics

        3. Topic hierarchy generation

        4. Quality metrics and visualization

        5. MongoDB integration support'
      metadata: {}
      dependencies:
      - Top2Vec
      - ValueError
      - all_words.update
      - df.iterrows
      - doc_probs.append
      - doc_topic_matrix.sum
      - doc_topics.append
      - enumerate
      - float
      - hasattr
      - int
      - isinstance
      - join
      - len
      - list
      - logging.getLogger
      - max
      - min
      - new_assignments.append
      - new_probs.append
      - np.argmax
      - np.dot
      - np.linalg.norm
      - np.mean
      - np.sqrt
      - np.zeros
      - pd.DataFrame
      - plt.annotate
      - plt.figure
      - plt.scatter
      - plt.show
      - plt.tight_layout
      - plt.title
      - plt.xlabel
      - plt.ylabel
      - print
      - range
      - re.sub
      - report_data.append
      - round
      - self._clean_text
      - self.get_document_topics
      - self.logger.error
      - self.logger.info
      - self.logger.warning
      - self.model.embed
      - self.model.get_documents_topics
      - self.model.get_num_topics
      - self.model.get_topic_sizes
      - self.model.get_topics
      - self.model.hierarchical_topic_reduction
      - set
      - similarities.append
      - sorted
      - str
      - strip
      - sum
      - text.lower
      - text.split
      - topic_avg_probs.get
      - topic_counts.get
      - topic_counts.keys
      - topic_counts.values
      - type
      - umap_model.transform
      - zip
      line: 17
    functions:
    - name: example_classification
      type: function
      parameters:
      - documents
      docstring: Example classification workflow
      metadata: {}
      dependencies:
      - EnhancedTopicModeler
      - print
      - topic_model.fit
      - topic_model.print_topic_report
      - topic_model.reassign_outliers
      - topic_model.topic_keywords_table
      - topic_model.visualize_topics
      line: 495
    filename: C:/Users/mad_p/OneDrive/Desktop/Py Projects/arxiv_pipeline/src/pipeline/top2vec.py
  dependencies: []
- name: src\pubmed\config
  filename: src\pubmed\config.py
  metadata: {}
  components: []
  dependencies: []
- name: src\pubmed\db_client
  filename: src\pubmed\db_client.py
  metadata: {}
  components:
  - functions:
    - name: article_exists
      type: function
      parameters:
      - pmid
      docstring: ''
      metadata: {}
      dependencies:
      - collection.find_one
      line: 9
    - name: save_article
      type: function
      parameters:
      - data
      docstring: ''
      metadata: {}
      dependencies:
      - collection.insert_one
      - isinstance
      line: 12
    - name: save_articles
      type: function
      parameters:
      - data_list
      docstring: ''
      metadata: {}
      dependencies:
      - collection.insert_many
      line: 16
    - name: get_article
      type: function
      parameters:
      - pmid
      docstring: ''
      metadata: {}
      dependencies:
      - collection.find_one
      line: 20
    filename: C:/Users/mad_p/OneDrive/Desktop/Py Projects/arxiv_pipeline/src/pubmed/db_client.py
  dependencies: []
- name: src\pubmed\main
  filename: src\pubmed\main.py
  metadata: {}
  components:
  - functions:
    - name: main
      type: function
      parameters: []
      docstring: ''
      metadata: {}
      dependencies:
      - argparse.ArgumentParser
      - article_exists
      - fetch_details
      - len
      - parser.add_argument
      - parser.parse_args
      - print
      - save_articles
      - search_pubmed
      line: 6
    filename: C:/Users/mad_p/OneDrive/Desktop/Py Projects/arxiv_pipeline/src/pubmed/main.py
  dependencies: []
- name: src\pubmed\pubmed_client
  filename: src\pubmed\pubmed_client.py
  metadata: {}
  components:
  - functions:
    - name: search_pubmed
      type: function
      parameters:
      - query
      - max_results
      docstring: ''
      metadata: {}
      dependencies:
      - Entrez.esearch
      - Entrez.read
      - handle.close
      line: 8
    - name: fetch_details
      type: function
      parameters:
      - pmid_list
      docstring: ''
      metadata: {}
      dependencies:
      - Entrez.efetch
      - handle.close
      - handle.read
      - join
      - parse_pubmed_xml
      line: 14
    - name: parse_pubmed_xml
      type: function
      parameters:
      - xml_data
      docstring: ''
      metadata: {}
      dependencies:
      - ET.fromstring
      - a.find
      - a.findtext
      - article.findall
      - article.findtext
      - articles.append
      - print
      - root.findall
      - strip
      line: 20
    filename: C:/Users/mad_p/OneDrive/Desktop/Py Projects/arxiv_pipeline/src/pubmed/pubmed_client.py
  dependencies: []
- name: src\storage\mongo
  filename: src\storage\mongo.py
  metadata: {}
  components:
  - classes:
    - name: MongoStorage
      type: class
      methods:
      - name: __init__
        type: method
        parameters:
        - self
        - connection_string
        - db_name
        docstring: "Initialize MongoDB connection.\n\nArgs:\n    connection_string:\
          \ MongoDB connection URI\n    db_name: Target database name"
        metadata: {}
        dependencies:
        - MongodbLoader
        - pymongo.MongoClient
        - self._setup_indexes
        line: 15
      - name: _setup_indexes
        type: method
        parameters:
        - self
        docstring: Set up MongoDB indexes for optimized queries.
        metadata: {}
        dependencies:
        - logger.info
        - self.papers.create_index
        line: 42
      - name: store_papers
        type: method
        parameters:
        - self
        - papers
        docstring: "Store papers in MongoDB with upsert to handle duplicates.\n\n\
          Args:\n    papers: List of paper metadata dictionaries\n\nReturns:\n   \
          \ Stats dictionary with counts of inserted and updated documents"
        metadata: {}
        dependencies:
        - datetime.utcnow
        - len
        - logger.error
        - logger.exception
        - logger.info
        - logger.warning
        - paper.get
        - pymongo.errors.PyMongoError
        - self.papers.update_one
        - self.stats.insert_one
        - str
        line: 50
      - name: store_papers_bulk
        type: method
        parameters:
        - self
        - papers
        docstring: "Bulk upsert papers for efficiency with large batches.\n\nArgs:\n\
          \    papers: List of paper metadata dictionaries\n\nReturns:\n    Stats\
          \ dictionary with counts of inserted and updated documents"
        metadata: {}
        dependencies:
        - UpdateOne
        - datetime.utcnow
        - len
        - logger.error
        - logger.exception
        - logger.info
        - logger.warning
        - operations.append
        - pymongo.UpdateOne
        - pymongo.errors.BulkWriteError
        - pymongo.errors.PyMongoError
        - self.papers.bulk_write
        - self.stats.insert_one
        - str
        line: 104
      - name: get_paper
        type: method
        parameters:
        - self
        - paper_id
        docstring: Retrieve single paper by ID.
        metadata: {}
        dependencies:
        - self.loader.load
        line: 161
      - name: get_papers
        type: method
        parameters:
        - self
        - filter_query
        - limit
        - skip
        - sort_by
        - sort_order
        docstring: "Retrieve papers with filtering, pagination and sorting.\n\nArgs:\n\
          \    filter_query: MongoDB filter query\n    limit: Max number of results\n\
          \    skip: Number of documents to skip (pagination)\n    sort_by: Field\
          \ to sort by\n    sort_order: pymongo.ASCENDING (1) or pymongo.DESCENDING\
          \ (-1)\n\nReturns:\n    List of paper documents"
        metadata: {}
        dependencies:
        - self.loader.load
        line: 169
      - name: get_stats
        type: method
        parameters:
        - self
        - limit
        docstring: Get recent ingestion statistics.
        metadata: {}
        dependencies:
        - limit
        - list
        - self.stats.find
        - sort
        line: 202
      - name: close
        type: method
        parameters:
        - self
        docstring: Close MongoDB connection.
        metadata: {}
        dependencies:
        - self.client.close
        line: 206
      - name: __enter__
        type: method
        parameters:
        - self
        docstring: Enable use as a context manager.
        metadata: {}
        dependencies: []
        line: 210
      - name: __exit__
        type: method
        parameters:
        - self
        - exc_type
        - exc_val
        - exc_tb
        docstring: Ensure connection is closed when exiting context.
        metadata: {}
        dependencies:
        - self.close
        line: 214
      docstring: MongoDB storage for arXiv papers, using LangChain.
      metadata: {}
      dependencies:
      - MongodbLoader
      - UpdateOne
      - datetime.utcnow
      - len
      - limit
      - list
      - logger.error
      - logger.exception
      - logger.info
      - logger.warning
      - operations.append
      - paper.get
      - pymongo.MongoClient
      - pymongo.UpdateOne
      - pymongo.errors.BulkWriteError
      - pymongo.errors.PyMongoError
      - self._setup_indexes
      - self.client.close
      - self.close
      - self.loader.load
      - self.papers.bulk_write
      - self.papers.create_index
      - self.papers.update_one
      - self.stats.find
      - self.stats.insert_one
      - sort
      - str
      line: 12
    filename: C:/Users/mad_p/OneDrive/Desktop/Py Projects/arxiv_pipeline/src/storage/mongo.py
  dependencies: []
- name: src\utils\analyze_papers_by_year_month_day
  filename: src\utils\analyze_papers_by_year_month_day.py
  metadata: {}
  components:
  - functions:
    - name: analyze_papers_by_year_month_day
      type: function
      parameters:
      - connection_string
      - db_name
      - start_date
      - end_date
      - year_filter
      - category
      docstring: "Query MongoDB and analyze papers hierarchically by year, month,\
        \ and day.\n\nArgs:\n    connection_string: MongoDB connection URI (if None,\
        \ uses MONGO_URI env var or default)\n    db_name: MongoDB database name\n\
        \    start_date: Optional start date filter (format: YYYY-MM-DD)\n    end_date:\
        \ Optional end date filter (format: YYYY-MM-DD)\n    year_filter: Optional\
        \ year to filter results (e.g., 2024)\n    category: Optional category filter\
        \ (e.g., 'cs.AI' or 'math.ST')\n    \nReturns:\n    Tuple of (yearly_data,\
        \ monthly_data, daily_data, total_papers, categories_list)"
      metadata: {}
      dependencies:
      - MongoStorage
      - OrderedDict
      - daily_data.items
      - defaultdict
      - doc.get
      - get
      - logger.error
      - logger.info
      - mongo.papers.aggregate
      - mongo.papers.count_documents
      - monthly_data.items
      - os.environ.get
      - sorted
      - str
      - yearly_data.items
      line: 27
    - name: display_yearly_summary
      type: function
      parameters:
      - yearly_data
      - total_papers
      docstring: Display paper counts by year with visualization.
      metadata: {}
      dependencies:
      - int
      - max
      - print
      - yearly_data.items
      - yearly_data.values
      line: 130
    - name: display_monthly_summary
      type: function
      parameters:
      - monthly_data
      - total_papers
      - year_filter
      docstring: Display paper counts by year-month with visualization.
      metadata: {}
      dependencies:
      - OrderedDict
      - filtered_data.items
      - filtered_data.values
      - int
      - max
      - monthly_data.items
      - print
      - year_month.startswith
      line: 152
    - name: display_daily_summary
      type: function
      parameters:
      - daily_data
      - total_papers
      - year_month_filter
      - max_days
      docstring: Display paper counts by full date with visualization.
      metadata: {}
      dependencies:
      - OrderedDict
      - daily_data.items
      - date.startswith
      - date_obj.strftime
      - datetime.strptime
      - filtered_data.items
      - filtered_data.values
      - int
      - len
      - list
      - max
      - print
      line: 185
    - name: calculate_day_of_week_stats
      type: function
      parameters:
      - daily_data
      docstring: Calculate paper counts by day of week.
      metadata: {}
      dependencies:
      - OrderedDict
      - daily_data.items
      - date_obj.weekday
      - datetime.strptime
      - defaultdict
      - range
      line: 230
    - name: display_day_of_week_stats
      type: function
      parameters:
      - day_of_week_data
      - total_papers
      docstring: Display paper counts by day of week with visualization.
      metadata: {}
      dependencies:
      - day_of_week_data.items
      - day_of_week_data.values
      - int
      - max
      - print
      line: 258
    - name: main
      type: function
      parameters: []
      docstring: Main entry point for the script.
      metadata: {}
      dependencies:
      - analyze_papers_by_year_month_day
      - argparse.ArgumentParser
      - calculate_day_of_week_stats
      - display_daily_summary
      - display_day_of_week_stats
      - display_monthly_summary
      - display_yearly_summary
      - len
      - logger.error
      - os.environ.get
      - parser.add_argument
      - parser.parse_args
      - print
      - str
      line: 280
    filename: C:/Users/mad_p/OneDrive/Desktop/Py Projects/arxiv_pipeline/src/utils/analyze_papers_by_year_month_day.py
  dependencies: []
- name: src\utils\check_mongodb
  filename: src\utils\check_mongodb.py
  metadata: {}
  components:
  - functions:
    - name: main
      type: function
      parameters: []
      docstring: ''
      metadata: {}
      dependencies:
      - MongoClient
      - db.papers.count_documents
      - db.papers.find_one
      - print
      - sample.get
      line: 3
    filename: C:/Users/mad_p/OneDrive/Desktop/Py Projects/arxiv_pipeline/src/utils/check_mongodb.py
  dependencies: []
- name: src\utils\check_pdf_urls
  filename: src\utils\check_pdf_urls.py
  metadata: {}
  components:
  - functions:
    - name: load_config
      type: function
      parameters: []
      docstring: Load configuration from default.yaml file.
      metadata: {}
      dependencies:
      - open
      - os.path.dirname
      - os.path.join
      - yaml.safe_load
      line: 10
    - name: check_pdf_urls
      type: function
      parameters: []
      docstring: Check MongoDB for papers with /pdf/ in the pdf_url and analyze downloaded
        files.
      metadata: {}
      dependencies:
      - MongoClient
      - client.close
      - downloaded_pdfs_collection.find
      - downloaded_with_pattern.append
      - enumerate
      - join
      - len
      - list
      - load_config
      - logger.info
      - not_downloaded_with_pattern.append
      - os.getenv
      - os.path.exists
      - os.path.join
      - paper.get
      - papers_collection.find
      - pdf_url.rstrip
      - split
      line: 16
    filename: C:/Users/mad_p/OneDrive/Desktop/Py Projects/arxiv_pipeline/src/utils/check_pdf_urls.py
  dependencies: []
- name: src\utils\check_topics
  filename: src\utils\check_topics.py
  metadata: {}
  components:
  - functions:
    - name: load_config
      type: function
      parameters:
      - config_path
      docstring: ''
      metadata: {}
      dependencies:
      - open
      - yaml.safe_load
      line: 6
    - name: main
      type: function
      parameters: []
      docstring: ''
      metadata: {}
      dependencies:
      - MongoClient
      - list
      - load_config
      - print
      - topics_collection.aggregate
      - topics_collection.count_documents
      line: 10
    filename: C:/Users/mad_p/OneDrive/Desktop/Py Projects/arxiv_pipeline/src/utils/check_topics.py
  dependencies: []
- name: src\utils\clear_neo4j
  filename: src\utils\clear_neo4j.py
  metadata: {}
  components:
  - functions:
    - name: load_config
      type: function
      parameters:
      - path
      docstring: Load configuration from YAML file
      metadata: {}
      dependencies:
      - logger.error
      - open
      - yaml.safe_load
      line: 43
    - name: clear_neo4j
      type: function
      parameters:
      - uri
      - user
      - password
      - dry_run
      docstring: "Clear all data from Neo4j database\n\nArgs:\n    uri: Neo4j connection\
        \ URI\n    user: Neo4j username\n    password: Neo4j password\n    dry_run:\
        \ If True, only show what would be deleted without actually deleting"
      metadata: {}
      dependencies:
      - GraphDatabase.driver
      - driver.close
      - driver.session
      - locals
      - logger.error
      - logger.info
      - result.single
      - session.run
      line: 52
    - name: main
      type: function
      parameters: []
      docstring: ''
      metadata: {}
      dependencies:
      - argparse.ArgumentParser
      - clear_neo4j
      - load_config
      - parser.add_argument
      - parser.parse_args
      line: 99
    filename: C:/Users/mad_p/OneDrive/Desktop/Py Projects/arxiv_pipeline/src/utils/clear_neo4j.py
  dependencies: []
- name: src\utils\count_papers_by_date
  filename: src\utils\count_papers_by_date.py
  metadata: {}
  components:
  - functions:
    - name: count_papers_by_published_date
      type: function
      parameters:
      - connection_string
      - db_name
      docstring: "Query MongoDB and count papers grouped by published date.\n\nArgs:\n\
        \    connection_string: MongoDB connection URI (if None, uses MONGO_URI env\
        \ var or default)\n    db_name: MongoDB database name\n    \nReturns:\n  \
        \  Ordered dictionary of publication dates and paper counts"
      metadata: {}
      dependencies:
      - MongoStorage
      - OrderedDict
      - len
      - logger.error
      - logger.info
      - mongo.papers.aggregate
      - os.environ.get
      - str
      line: 28
    - name: display_results
      type: function
      parameters:
      - counts
      docstring: "Display paper counts in a formatted way.\n\nArgs:\n    counts: Ordered\
        \ dictionary of dates and counts"
      metadata: {}
      dependencies:
      - counts.items
      - counts.values
      - list
      - print
      - reversed
      - sum
      line: 73
    - name: main
      type: function
      parameters: []
      docstring: Main entry point for the script.
      metadata: {}
      dependencies:
      - OrderedDict
      - count_papers_by_published_date
      - counts.items
      - display_results
      - len
      - list
      - logger.error
      - logger.info
      - monthly.get
      - monthly.items
      - monthly.values
      - os.environ.get
      - print
      - reversed
      - str
      - sum
      line: 99
    filename: C:/Users/mad_p/OneDrive/Desktop/Py Projects/arxiv_pipeline/src/utils/count_papers_by_date.py
  dependencies: []
- name: src\utils\download_pdfs
  filename: src\utils\download_pdfs.py
  metadata: {}
  components:
  - functions:
    - name: load_config
      type: function
      parameters: []
      docstring: ''
      metadata: {}
      dependencies:
      - open
      - os.path.dirname
      - os.path.join
      - yaml.safe_load
      line: 9
    - name: ensure_dir
      type: function
      parameters:
      - directory
      docstring: ''
      metadata: {}
      dependencies:
      - os.makedirs
      - os.path.exists
      line: 42
    - name: extract_arxiv_id
      type: function
      parameters:
      - pdf_url
      docstring: 'Extract arXiv ID from a PDF URL.

        Example: http://arxiv.org/pdf/2504.18538v1 -> 2504.18538v1'
      metadata: {}
      dependencies:
      - pdf_url.rstrip
      - split
      line: 46
    - name: get_pdf_filename
      type: function
      parameters:
      - paper
      docstring: ''
      metadata: {}
      dependencies:
      - extract_arxiv_id
      - paper.get
      line: 55
    - name: download_pdf
      type: function
      parameters:
      - url
      - filepath
      - db
      docstring: ''
      metadata: {}
      dependencies:
      - datetime.utcnow
      - extract_arxiv_id
      - f.write
      - invalid_pdfs.find_one
      - invalid_pdfs.insert_one
      - open
      - print
      - requests.get
      - response.iter_content
      - response.raise_for_status
      line: 61
    - name: filter_papers_by_date
      type: function
      parameters:
      - papers_cursor
      - start_date
      - end_date
      docstring: 'Filter papers by published date (ISO format: YYYY-MM-DD).'
      metadata: {}
      dependencies:
      - datetime.datetime
      - datetime.fromisoformat
      - filtered_papers.append
      line: 87
    - name: sort_papers_by_date
      type: function
      parameters:
      - papers
      - ascending
      docstring: Sort papers by published date.
      metadata: {}
      dependencies:
      - datetime.datetime
      - datetime.fromisoformat
      - paper.get
      - sorted
      line: 122
    - name: get_date
      type: function
      parameters:
      - paper
      docstring: ''
      metadata: {}
      dependencies:
      - datetime.fromisoformat
      - paper.get
      line: 126
    - name: get_downloaded_papers_from_db
      type: function
      parameters:
      - db
      docstring: 'Fetch previously downloaded papers from both downloaded_pdfs and
        invalid_pdfs collections.

        Returns a set of arXiv IDs that should be skipped.'
      metadata: {}
      dependencies:
      - db.get_collection
      - db.list_collection_names
      - downloaded_ids.update
      - downloaded_pdfs_collection.find
      - invalid_pdfs_collection.find
      - print
      - set
      line: 134
    - name: main
      type: function
      parameters: []
      docstring: Main function to download PDFs from arXiv.
      metadata: {}
      dependencies:
      - MongoClient
      - categories_found.add
      - category_counts.items
      - download_pdf
      - ensure_dir
      - extract_arxiv_id
      - filter_papers_by_date
      - find
      - get_downloaded_papers_from_db
      - get_pdf_filename
      - len
      - list
      - os.path.exists
      - os.path.join
      - paper.get
      - print
      - set
      - sort
      - sort_papers_by_date
      - tqdm
      line: 157
    filename: C:/Users/mad_p/OneDrive/Desktop/Py Projects/arxiv_pipeline/src/utils/download_pdfs.py
  dependencies: []
- name: src\utils\inspect_paper_schema
  filename: src\utils\inspect_paper_schema.py
  metadata: {}
  components:
  - functions:
    - name: inspect_paper_schema
      type: function
      parameters: []
      docstring: Examine the structure of a paper document in MongoDB.
      metadata: {}
      dependencies:
      - MongoStorage
      - isinstance
      - json.dumps
      - len
      - list
      - os.environ.get
      - paper.get
      - paper.items
      - print
      - storage.close
      - storage.papers.aggregate
      - storage.papers.count_documents
      - storage.papers.find_one
      - str
      - type
      line: 18
    filename: C:/Users/mad_p/OneDrive/Desktop/Py Projects/arxiv_pipeline/src/utils/inspect_paper_schema.py
  dependencies: []
- name: src\utils\load_huggingface_metadata
  filename: src\utils\load_huggingface_metadata.py
  metadata: {}
  components:
  - functions:
    - name: fetch_models
      type: function
      parameters:
      - limit
      - filters
      docstring: ''
      metadata: {}
      dependencies:
      - params.update
      - requests.get
      - response.json
      - response.raise_for_status
      line: 16
    - name: extract_model_info
      type: function
      parameters:
      - model_data
      docstring: ''
      metadata: {}
      dependencies:
      - card_data.get
      - extracted.append
      - model.get
      line: 27
    - name: save_to_json
      type: function
      parameters:
      - data
      - filename
      docstring: ''
      metadata: {}
      dependencies:
      - datetime.utcnow
      - isoformat
      - json.dump
      - open
      line: 45
    - name: main
      type: function
      parameters: []
      docstring: ''
      metadata: {}
      dependencies:
      - extract_model_info
      - fetch_models
      - len
      - print
      - save_to_json
      line: 49
    filename: C:/Users/mad_p/OneDrive/Desktop/Py Projects/arxiv_pipeline/src/utils/load_huggingface_metadata.py
  dependencies: []
- name: src\utils\load_ollama_all_metadata
  filename: src\utils\load_ollama_all_metadata.py
  metadata: {}
  components:
  - functions:
    - name: fetch_ollama_available_models
      type: function
      parameters: []
      docstring: ''
      metadata: {}
      dependencies:
      - BeautifulSoup
      - Exception
      - card.text.strip
      - models.append
      - requests.get
      - soup.select
      - split
      - strip
      line: 5
    filename: C:/Users/mad_p/OneDrive/Desktop/Py Projects/arxiv_pipeline/src/utils/load_ollama_all_metadata.py
  dependencies: []
- name: src\utils\load_ollama_metadata
  filename: src\utils\load_ollama_metadata.py
  metadata: {}
  components:
  - functions:
    - name: fetch_ollama_models
      type: function
      parameters: []
      docstring: ''
      metadata: {}
      dependencies:
      - Exception
      - get
      - model.get
      - models.append
      - requests.get
      - response.json
      line: 4
    filename: C:/Users/mad_p/OneDrive/Desktop/Py Projects/arxiv_pipeline/src/utils/load_ollama_metadata.py
  dependencies: []
- name: src\utils\logger
  filename: src\utils\logger.py
  metadata: {}
  components:
  - functions:
    - name: setup_logger
      type: function
      parameters:
      - name
      - log_level
      docstring: ''
      metadata: {}
      dependencies:
      - RotatingFileHandler
      - console_handler.setFormatter
      - console_handler.setLevel
      - file_handler.setFormatter
      - file_handler.setLevel
      - logger.addHandler
      - logger.setLevel
      - logging.Formatter
      - logging.StreamHandler
      - logging.getLogger
      - os.makedirs
      - os.path.dirname
      - os.path.exists
      - os.path.join
      line: 5
    filename: C:/Users/mad_p/OneDrive/Desktop/Py Projects/arxiv_pipeline/src/utils/logger.py
  dependencies: []
- name: src\utils\query_top2vec_topics
  filename: src\utils\query_top2vec_topics.py
  metadata: {}
  components:
  - functions:
    - name: load_config
      type: function
      parameters:
      - config_path
      docstring: Load configuration settings from a YAML file.
      metadata: {}
      dependencies:
      - open
      - yaml.safe_load
      line: 25
    - name: is_docker
      type: function
      parameters: []
      docstring: Detect if running inside a Docker container.
      metadata: {}
      dependencies:
      - f.read
      - open
      line: 30
    - name: get_mongo_uri
      type: function
      parameters:
      - config
      docstring: Get the appropriate MongoDB URI based on environment and configuration.
      metadata: {}
      dependencies:
      - is_docker
      - os.environ.get
      line: 38
    - name: query_topics
      type: function
      parameters:
      - config
      docstring: Query the top2vec topics collection and display statistics.
      metadata: {}
      dependencies:
      - MongoClient
      - defaultdict
      - get_mongo_uri
      - join
      - len
      - list
      - logger.error
      - logger.info
      - logger.warning
      - max
      - mean
      - min
      - pd.DataFrame
      - print
      - rows.append
      - str
      - tabulate
      - topic_words_dict.get
      - topics_collection.aggregate
      - topics_collection.count_documents
      - topics_collection.find_one
      line: 50
    - name: main
      type: function
      parameters: []
      docstring: Main entry point for the query script.
      metadata: {}
      dependencies:
      - argparse.ArgumentParser
      - load_config
      - logger.info
      - parser.add_argument
      - parser.parse_args
      - query_topics
      line: 115
    filename: C:/Users/mad_p/OneDrive/Desktop/Py Projects/arxiv_pipeline/src/utils/query_top2vec_topics.py
  dependencies: []
- name: src\utils\track_downloaded_pdfs
  filename: src\utils\track_downloaded_pdfs.py
  metadata: {}
  components:
  - functions:
    - name: load_config
      type: function
      parameters: []
      docstring: Load configuration from default.yaml file.
      metadata: {}
      dependencies:
      - open
      - os.path.dirname
      - os.path.join
      - yaml.safe_load
      line: 13
    - name: ensure_collection
      type: function
      parameters:
      - db
      - collection_name
      docstring: Ensure collection exists in MongoDB.
      metadata: {}
      dependencies:
      - db.create_collection
      - db.list_collection_names
      - logger.info
      line: 19
    - name: get_arxiv_id_from_filename
      type: function
      parameters:
      - filename
      docstring: 'Extract arXiv ID from filename.

        Example: 2504.18538v1.pdf -> 2504.18538v1'
      metadata: {}
      dependencies:
      - filename.endswith
      line: 25
    - name: scan_download_directory
      type: function
      parameters:
      - pdf_dir
      - process_categories
      docstring: 'Scan the download directory for PDF files.

        Returns a dictionary mapping arXiv IDs to their categories.'
      metadata: {}
      dependencies:
      - f.endswith
      - get_arxiv_id_from_filename
      - logger.error
      - logger.warning
      - os.listdir
      - os.path.exists
      - os.path.isdir
      - os.path.join
      line: 33
    - name: update_mongodb
      type: function
      parameters:
      - mongo_client
      - db_name
      - collection_name
      - downloaded_pdfs
      docstring: 'Update MongoDB collection with downloaded PDFs information.

        - Adds entries for new downloads

        - Updates entries for existing downloads

        - Marks entries as not downloaded if PDF is no longer present'
      metadata: {}
      dependencies:
      - bulk_operations.append
      - collection.bulk_write
      - collection.find
      - collection.update_many
      - datetime.utcnow
      - downloaded_pdfs.items
      - ensure_collection
      - isoformat
      - len
      - list
      - logger.error
      - logger.info
      - min
      - pymongo.InsertOne
      - pymongo.UpdateOne
      - range
      - record.get
      - to_mark_as_removed.append
      line: 70
    - name: main
      type: function
      parameters: []
      docstring: Main function to track downloaded PDFs.
      metadata: {}
      dependencies:
      - MongoClient
      - client.admin.command
      - client.close
      - get
      - join
      - len
      - load_config
      - locals
      - logger.error
      - logger.info
      - os.getenv
      - scan_download_directory
      - update_mongodb
      line: 192
    filename: C:/Users/mad_p/OneDrive/Desktop/Py Projects/arxiv_pipeline/src/utils/track_downloaded_pdfs.py
  dependencies: []
- name: src\utils\validate_mongodb_data
  filename: src\utils\validate_mongodb_data.py
  metadata: {}
  components:
  - functions:
    - name: run_validation
      type: function
      parameters:
      - connection_string
      - db_name
      - sample_size
      - date_range
      docstring: "Run comprehensive data validation and analysis on the MongoDB papers\
        \ collection.\n\nArgs:\n    connection_string: MongoDB connection URI (if\
        \ None, uses MONGO_URI env var or default)\n    db_name: MongoDB database\
        \ name (if None, uses MONGO_DB env var or default)\n    sample_size: Number\
        \ of documents to validate\n    date_range: Optional tuple of (start_date,\
        \ end_date) strings"
      metadata: {}
      dependencies:
      - MongoStorage
      - analyze_mongodb_collection
      - check_data_integrity
      - count_papers_by_date
      - dict
      - enumerate
      - generate_date_distribution_report
      - integrity_checks.get
      - items
      - join
      - keys
      - len
      - list
      - logger.info
      - monthly_counts.items
      - os.environ.get
      - print
      - sorted
      - validate_mongodb_data
      line: 33
    - name: main
      type: function
      parameters: []
      docstring: Main entry point for the script.
      metadata: {}
      dependencies:
      - argparse.ArgumentParser
      - parser.add_argument
      - parser.parse_args
      - run_validation
      line: 153
    filename: C:/Users/mad_p/OneDrive/Desktop/Py Projects/arxiv_pipeline/src/utils/validate_mongodb_data.py
  dependencies: []
- name: tests\test_ingestion
  filename: tests\test_ingestion.py
  metadata: {}
  components: []
  dependencies: []
- name: tests\test_storage
  filename: tests\test_storage.py
  metadata: {}
  components: []
  dependencies: []
entry_points: []
external_connections: []
