{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ArXiv Pipeline - MongoDB Analysis\n",
    "\n",
    "This notebook explores and visualizes the paper metadata stored in MongoDB.\n",
    "\n",
    "Key analyses include:\n",
    "- Paper publication trends over time\n",
    "- Author analytics and rankings\n",
    "- Category distribution analysis \n",
    "- Citation metrics\n",
    "- Text analysis of titles/abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pymongo import MongoClient\n",
    "from collections import Counter\n",
    "from dotenv import load_dotenv\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "# Load environment variables from .env file (if present)\n",
    "load_dotenv()\n",
    "\n",
    "# Set Matplotlib config\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# MongoDB Configuration\n",
    "MONGO_URI = os.getenv(\"MONGO_URI\", \"mongodb://localhost:27017/\")\n",
    "MONGO_DB = os.getenv(\"MONGO_DB\", \"arxiv_papers\")\n",
    "\n",
    "print(f\"MongoDB URI: {MONGO_URI}\")\n",
    "print(f\"MongoDB Database: {MONGO_DB}\")\n",
    "\n",
    "# Connect to MongoDB\n",
    "try:\n",
    "    client = MongoClient(MONGO_URI)\n",
    "    db = client[MONGO_DB]\n",
    "    # Test connection\n",
    "    server_info = client.server_info()\n",
    "    print(f\"✅ Connected to MongoDB (version: {server_info.get('version')})\")\n",
    "    \n",
    "    # Get collection stats\n",
    "    print(\"\\nCollection statistics:\")\n",
    "    collections = db.list_collection_names()\n",
    "    for collection in collections:\n",
    "        count = db[collection].count_documents({})\n",
    "        print(f\"- {collection}: {count:,} documents\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Failed to connect to MongoDB: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Paper Publication Trends Over Time\n",
    "\n",
    "Let's analyze how the publication volume has changed over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Convert to DataFrame for time series analysis\n",
    "papers_df = pd.DataFrame(list(db.papers.find({}, {'_id': 1, 'id': 1, 'title': 1, 'authors': 1, 'categories': 1, 'update_date': 1, 'published': 1})))\n",
    "\n",
    "# Convert date strings to datetime objects\n",
    "papers_df['published_date'] = pd.to_datetime(papers_df['published'], errors='coerce')\n",
    "papers_df['year'] = papers_df['published_date'].dt.year\n",
    "papers_df['month'] = papers_df['published_date'].dt.month\n",
    "papers_df['year_month'] = papers_df['published_date'].dt.strftime('%Y-%m')\n",
    "\n",
    "# Get papers per year\n",
    "papers_per_year = papers_df['year'].value_counts().sort_index()\n",
    "\n",
    "# Plot papers per year\n",
    "plt.figure(figsize=(12, 6))\n",
    "ax = papers_per_year.plot(kind='bar', color='steelblue')\n",
    "plt.title('Number of Papers Published per Year', fontsize=16)\n",
    "plt.xlabel('Year', fontsize=14)\n",
    "plt.ylabel('Number of Papers', fontsize=14)\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Add data labels on top of each bar\n",
    "for i, v in enumerate(papers_per_year):\n",
    "    ax.text(i, v + 10, f\"{v:,}\", ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Monthly trend for the last 3 years (if data spans that far)\n",
    "current_year = datetime.now().year\n",
    "three_years_ago = current_year - 3\n",
    "\n",
    "recent_monthly = papers_df[papers_df['year'] >= three_years_ago]\n",
    "monthly_counts = recent_monthly['year_month'].value_counts().sort_index()\n",
    "\n",
    "if not monthly_counts.empty:\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    monthly_counts.plot(kind='line', marker='o', linewidth=2, markersize=8)\n",
    "    plt.title(f'Monthly Paper Publications ({three_years_ago}-{current_year})', fontsize=16)\n",
    "    plt.xlabel('Year-Month', fontsize=14)\n",
    "    plt.ylabel('Number of Papers', fontsize=14)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No data available for the last three years.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Author Analytics\n",
    "\n",
    "Let's analyze the most prolific authors and co-authorship patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Function to extract all authors from papers\n",
    "def extract_authors(papers_df):\n",
    "    all_authors = []\n",
    "    author_paper_count = Counter()\n",
    "    \n",
    "    for _, paper in papers_df.iterrows():\n",
    "        if 'authors' in paper and paper['authors']:\n",
    "            paper_authors = paper['authors']\n",
    "            # Add to all authors list\n",
    "            all_authors.extend(paper_authors)\n",
    "            # Count papers per author\n",
    "            for author in paper_authors:\n",
    "                author_paper_count[author] += 1\n",
    "    \n",
    "    return all_authors, author_paper_count\n",
    "\n",
    "# Get author data\n",
    "all_authors, author_paper_count = extract_authors(papers_df)\n",
    "\n",
    "# Most prolific authors (top 20)\n",
    "top_authors = pd.DataFrame(author_paper_count.most_common(20), \n",
    "                           columns=['Author', 'Paper Count'])\n",
    "\n",
    "# Plot top authors\n",
    "plt.figure(figsize=(14, 8))\n",
    "bars = plt.barh(top_authors['Author'], top_authors['Paper Count'], color='steelblue')\n",
    "plt.title('Top 20 Most Prolific Authors', fontsize=16)\n",
    "plt.xlabel('Number of Papers', fontsize=14)\n",
    "plt.ylabel('Author', fontsize=14)\n",
    "plt.gca().invert_yaxis()  # Invert y-axis to have highest count at the top\n",
    "\n",
    "# Add count labels\n",
    "for bar in bars:\n",
    "    width = bar.get_width()\n",
    "    plt.text(width + 0.5, bar.get_y() + bar.get_height()/2, \n",
    "             f\"{width:,}\", ha='left', va='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Co-authorship analysis: distribution of authors per paper\n",
    "papers_df['author_count'] = papers_df['authors'].apply(lambda x: len(x) if isinstance(x, list) else 0)\n",
    "author_count_dist = papers_df['author_count'].value_counts().sort_index()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "ax = author_count_dist.plot(kind='bar', color='lightseagreen')\n",
    "plt.title('Distribution of Number of Authors per Paper', fontsize=16)\n",
    "plt.xlabel('Number of Authors', fontsize=14)\n",
    "plt.ylabel('Number of Papers', fontsize=14)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Add data labels\n",
    "for i, v in enumerate(author_count_dist):\n",
    "    ax.text(i, v + 10, f\"{v:,}\", ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate and display average authors per paper\n",
    "avg_authors = papers_df['author_count'].mean()\n",
    "print(f\"Average number of authors per paper: {avg_authors:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Category Distribution Analysis\n",
    "\n",
    "Let's analyze the distribution of categories across papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Function to extract all categories from papers\n",
    "def extract_categories(papers_df):\n",
    "    all_categories = []\n",
    "    category_paper_count = Counter()\n",
    "    \n",
    "    for _, paper in papers_df.iterrows():\n",
    "        if 'categories' in paper and paper['categories']:\n",
    "            paper_cats = paper['categories']\n",
    "            # Add to all categories list\n",
    "            all_categories.extend(paper_cats)\n",
    "            # Count papers per category\n",
    "            for cat in paper_cats:\n",
    "                category_paper_count[cat] += 1\n",
    "    \n",
    "    return all_categories, category_paper_count\n",
    "\n",
    "# Get category data\n",
    "all_categories, category_paper_count = extract_categories(papers_df)\n",
    "\n",
    "# Top categories (top 20)\n",
    "top_categories = pd.DataFrame(category_paper_count.most_common(20), \n",
    "                             columns=['Category', 'Paper Count'])\n",
    "\n",
    "# Plot top categories\n",
    "plt.figure(figsize=(14, 10))\n",
    "bars = plt.barh(top_categories['Category'], top_categories['Paper Count'], color='darkorange')\n",
    "plt.title('Top 20 Most Common Categories', fontsize=16)\n",
    "plt.xlabel('Number of Papers', fontsize=14)\n",
    "plt.ylabel('Category', fontsize=14)\n",
    "plt.gca().invert_yaxis()  # Invert y-axis to have highest count at the top\n",
    "\n",
    "# Add count labels\n",
    "for bar in bars:\n",
    "    width = bar.get_width()\n",
    "    plt.text(width + 0.5, bar.get_y() + bar.get_height()/2, \n",
    "             f\"{width:,}\", ha='left', va='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Distribution of categories per paper\n",
    "papers_df['category_count'] = papers_df['categories'].apply(lambda x: len(x) if isinstance(x, list) else 0)\n",
    "category_count_dist = papers_df['category_count'].value_counts().sort_index()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = category_count_dist.plot(kind='bar', color='purple')\n",
    "plt.title('Distribution of Number of Categories per Paper', fontsize=16)\n",
    "plt.xlabel('Number of Categories', fontsize=14)\n",
    "plt.ylabel('Number of Papers', fontsize=14)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Add data labels\n",
    "for i, v in enumerate(category_count_dist):\n",
    "    ax.text(i, v + 10, f\"{v:,}\", ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate and display average categories per paper\n",
    "avg_categories = papers_df['category_count'].mean()\n",
    "print(f\"Average number of categories per paper: {avg_categories:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Text Analysis of Titles and Abstracts\n",
    "\n",
    "Let's analyze the content of paper titles and abstracts to identify common terms and research trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Get sample of papers with abstracts for text analysis\n",
    "papers_with_abstract = list(db.papers.find({'abstract': {'$exists': True, '$ne': \"\"}}, \n",
    "                                          {'_id': 0, 'title': 1, 'abstract': 1, 'published': 1}))\n",
    "abstract_df = pd.DataFrame(papers_with_abstract)\n",
    "\n",
    "print(f\"Number of papers with abstracts: {len(abstract_df):,}\")\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    # Remove special chars and numbers\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    text = re.sub(r'\\d+', ' ', text)\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip().lower()\n",
    "    return text\n",
    "\n",
    "# Process titles\n",
    "abstract_df['clean_title'] = abstract_df['title'].apply(clean_text)\n",
    "all_titles = ' '.join(abstract_df['clean_title'].tolist())\n",
    "\n",
    "# Generate and display word cloud for titles\n",
    "plt.figure(figsize=(12, 8))\n",
    "wordcloud = WordCloud(width=800, height=400, \n",
    "                     background_color='white', \n",
    "                     collocations=False,\n",
    "                     max_words=100,\n",
    "                     contour_width=3).generate(all_titles)\n",
    "\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Word Cloud of Paper Titles\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Process abstracts if available\n",
    "if 'abstract' in abstract_df.columns and not abstract_df['abstract'].empty:\n",
    "    abstract_df['clean_abstract'] = abstract_df['abstract'].apply(clean_text)\n",
    "    all_abstracts = ' '.join(abstract_df['clean_abstract'].tolist())\n",
    "    \n",
    "    # Generate and display word cloud for abstracts\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    wordcloud = WordCloud(width=800, height=400, \n",
    "                         background_color='white', \n",
    "                         collocations=False,\n",
    "                         max_words=100,\n",
    "                         contour_width=3).generate(all_abstracts)\n",
    "    \n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Word Cloud of Paper Abstracts\", fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. MongoDB Collection Analysis\n",
    "\n",
    "Let's explore the structure and content of the ingestion_stats and processed_pdfs collections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Analyze ingestion_stats collection\n",
    "try:\n",
    "    # Get ingestion stats\n",
    "    ingestion_stats = list(db.ingestion_stats.find())\n",
    "    ingestion_df = pd.DataFrame(ingestion_stats)\n",
    "    \n",
    "    if not ingestion_df.empty:\n",
    "        print(f\"Ingestion stats collection contains {len(ingestion_df):,} records\")\n",
    "        \n",
    "        # Convert timestamp to datetime\n",
    "        if 'timestamp' in ingestion_df.columns:\n",
    "            ingestion_df['datetime'] = pd.to_datetime(ingestion_df['timestamp'], unit='s')\n",
    "            \n",
    "            # Plot ingestion over time\n",
    "            plt.figure(figsize=(14, 6))\n",
    "            ingestion_df.sort_values('datetime', inplace=True)\n",
    "            \n",
    "            # If we have papers_processed column\n",
    "            if 'papers_processed' in ingestion_df.columns:\n",
    "                plt.plot(ingestion_df['datetime'], ingestion_df['papers_processed'], \n",
    "                        marker='o', linestyle='-', linewidth=2, markersize=6)\n",
    "                plt.title('Papers Processed in Each Ingestion Run', fontsize=16)\n",
    "                plt.xlabel('Ingestion Date', fontsize=14)\n",
    "                plt.ylabel('Number of Papers Processed', fontsize=14)\n",
    "                plt.grid(True, linestyle='--', alpha=0.7)\n",
    "                plt.xticks(rotation=45)\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "            \n",
    "            # Show statistics\n",
    "            print(\"\\nIngestion Statistics Summary:\")\n",
    "            metrics = ['papers_processed', 'new_papers', 'updated_papers', 'execution_time']\n",
    "            for metric in metrics:\n",
    "                if metric in ingestion_df.columns:\n",
    "                    print(f\"- Total {metric.replace('_', ' ')}: {ingestion_df[metric].sum():,}\")\n",
    "                    print(f\"- Average {metric.replace('_', ' ')} per run: {ingestion_df[metric].mean():.2f}\")\n",
    "    else:\n",
    "        print(\"Ingestion stats collection is empty\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error analyzing ingestion_stats: {e}\")\n",
    "\n",
    "# Analyze processed_pdfs collection\n",
    "try:\n",
    "    # Get processed PDFs stats\n",
    "    processed_pdfs = list(db.processed_pdfs.find())\n",
    "    pdfs_df = pd.DataFrame(processed_pdfs)\n",
    "    \n",
    "    if not pdfs_df.empty:\n",
    "        print(f\"\\nProcessed PDFs collection contains {len(pdfs_df):,} records\")\n",
    "        \n",
    "        # Look at available fields\n",
    "        print(\"\\nProcessed PDFs Fields:\")\n",
    "        for col in pdfs_df.columns:\n",
    "            print(f\"- {col}\")\n",
    "            \n",
    "        # If we have processing metrics, analyze them\n",
    "        if 'processing_time' in pdfs_df.columns:\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            plt.hist(pdfs_df['processing_time'], bins=20, color='skyblue', edgecolor='black')\n",
    "            plt.title('Distribution of PDF Processing Times', fontsize=16)\n",
    "            plt.xlabel('Processing Time (seconds)', fontsize=14)\n",
    "            plt.ylabel('Number of PDFs', fontsize=14)\n",
    "            plt.grid(True, linestyle='--', alpha=0.7)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            print(f\"\\nAverage PDF processing time: {pdfs_df['processing_time'].mean():.2f} seconds\")\n",
    "            \n",
    "    else:\n",
    "        print(\"Processed PDFs collection is empty\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error analyzing processed_pdfs: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Database Health and Performance Metrics\n",
    "\n",
    "Let's check database health metrics and collection stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Database statistics\n",
    "try:\n",
    "    db_stats = db.command(\"dbStats\")\n",
    "    \n",
    "    # Extract key metrics\n",
    "    metrics = {\n",
    "        \"Collections\": db_stats['collections'],\n",
    "        \"Objects\": db_stats['objects'],\n",
    "        \"Data Size (MB)\": round(db_stats['dataSize'] / (1024 * 1024), 2),\n",
    "        \"Storage Size (MB)\": round(db_stats['storageSize'] / (1024 * 1024), 2),\n",
    "        \"Indexes\": db_stats['indexes'],\n",
    "        \"Index Size (MB)\": round(db_stats['indexSize'] / (1024 * 1024), 2)\n",
    "    }\n",
    "    \n",
    "    # Plot database stats\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Create a horizontal bar chart for size metrics\n",
    "    size_metrics = {k: v for k, v in metrics.items() if \"Size\" in k}\n",
    "    plt.subplot(2, 1, 1)\n",
    "    colors = plt.cm.viridis(np.linspace(0, 0.8, len(size_metrics)))\n",
    "    bars = plt.barh(list(size_metrics.keys()), list(size_metrics.values()), color=colors)\n",
    "    plt.title('Database Size Metrics', fontsize=14)\n",
    "    plt.xlabel('Size (MB)', fontsize=12)\n",
    "    \n",
    "    # Add size labels\n",
    "    for bar in bars:\n",
    "        width = bar.get_width()\n",
    "        plt.text(width + 0.5, bar.get_y() + bar.get_height()/2, \n",
    "                 f\"{width:,.2f} MB\", ha='left', va='center')\n",
    "    \n",
    "    # Create a bar chart for count metrics\n",
    "    count_metrics = {k: v for k, v in metrics.items() if \"Size\" not in k}\n",
    "    plt.subplot(2, 1, 2)\n",
    "    colors = plt.cm.plasma(np.linspace(0, 0.8, len(count_metrics)))\n",
    "    bars = plt.bar(list(count_metrics.keys()), list(count_metrics.values()), color=colors)\n",
    "    plt.title('Database Count Metrics', fontsize=14)\n",
    "    plt.ylabel('Count', fontsize=12)\n",
    "    \n",
    "    # Add count labels\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, height + 0.1, \n",
    "                 f\"{int(height):,}\", ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Get collection stats\n",
    "    print(\"\\nCollection Statistics:\")\n",
    "    collections = db.list_collection_names()\n",
    "    for collection in collections:\n",
    "        stats = db.command(\"collStats\", collection)\n",
    "        print(f\"\\n{collection}:\")\n",
    "        print(f\"- Document Count: {stats['count']:,}\")\n",
    "        print(f\"- Data Size: {stats['size'] / (1024 * 1024):.2f} MB\")\n",
    "        print(f\"- Storage Size: {stats['storageSize'] / (1024 * 1024):.2f} MB\")\n",
    "        print(f\"- Index Size: {stats['totalIndexSize'] / (1024 * 1024):.2f} MB\")\n",
    "        print(f\"- Avg Document Size: {stats.get('avgObjSize', 0) / 1024:.2f} KB\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error getting database statistics: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary and Conclusions\n",
    "\n",
    "Based on our analysis of the ArXiv paper metadata in MongoDB, we've observed:\n",
    "\n",
    "1. **Publication Trends**: [Add observation about publication trends]\n",
    "2. **Author Patterns**: [Add observation about author patterns]\n",
    "3. **Category Distribution**: [Add observation about category distribution]\n",
    "4. **Popular Topics**: [Add observation about popular topics from text analysis]\n",
    "5. **Database Performance**: [Add observation about database metrics]\n",
    "\n",
    "These insights can help inform research priorities and future enhancements to the ArXiv pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Close MongoDB connection\n",
    "client.close()\n",
    "print(\"MongoDB connection closed\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
