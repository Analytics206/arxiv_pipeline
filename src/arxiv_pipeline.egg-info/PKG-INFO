Metadata-Version: 2.4
Name: arxiv_pipeline
Version: 0.1.0
Summary: Local pipeline for processing ArXiv papers with graph and vector representation
License: MIT
Requires-Python: >=3.8
Description-Content-Type: text/markdown
Requires-Dist: requests>=2.28.0
Requires-Dist: pymongo>=4.3.0
Requires-Dist: neo4j>=5.5.0
Requires-Dist: pyyaml>=6.0
Requires-Dist: qdrant-client>=1.1.0
Requires-Dist: sentence-transformers>=2.2.0
Requires-Dist: pydantic>=2.0.0
Requires-Dist: langchain>=0.1.0
Requires-Dist: langchain-community>=0.0.30
Requires-Dist: tqdm>=4.64.0
Requires-Dist: pillow>=10.0.0
Requires-Dist: pymupdf>=1.23.0
Requires-Dist: motor>=3.3.0
Requires-Dist: transformers
Requires-Dist: torch
Requires-Dist: ollama
Requires-Dist: pypdf
Provides-Extra: dev
Requires-Dist: pytest>=7.0.0; extra == "dev"
Requires-Dist: black>=23.0.0; extra == "dev"
Requires-Dist: isort>=5.12.0; extra == "dev"
Requires-Dist: mypy>=1.0.0; extra == "dev"

# ArXiv Research Pipeline

ðŸ§  ArXiv Local AI Deep Research Pipeline
A modular, fully local, open-source pipeline for fetching, structuring, and exploring AI research papers from arXiv.org. It allows offline graph-based and semantic search through MongoDB, Neo4j, and Qdrant using Hugging Face embeddings. All services run in Docker for easy, consistent local deployment.

ðŸš€ Key Features
Local-first: Everything runs offlineâ€”no cloud dependencies but can deploy to cloud in containers. Fetching papers requires internet connection.

ArXiv Ingestion: Fetch non-duplicate papers saved locally from the cs.AI category (configurable) see list bottom of document.

MongoDB Storage: Stores structured and raw metadata.

Graph Representation: Neo4j graph database captures relationships between papers, authors, and categories.

LLM Category Summary: LLM reads papers to categories by subject, architecture and mathatical models.

Semantic Embeddings: Embeds text using Hugging Face models, stored in Qdrant for similarity search.

Configurable & Modular: Centralized settings let you switch categories, models, and components.

User Inferace: User friendly interface for explore datasets, knowledge graphs and similarity search.

Containerized: Fully Dockerized for isolated, repeatable setup with persistent Docker volumes for data storage.

![Image](https://github.com/user-attachments/assets/3233595b-ecbc-4029-a0f9-1e6723c026a7)

ðŸ“¦ System Components
| Component             | Purpose                                      |
| --------------------- | -------------------------------------------- |
| **Ingestion Service** | Fetches papers using arXiv Atom XML API      |
| **MongoDB**           | Stores raw and normalized metadata           |
| **Neo4j**             | Stores the author-paper-category graph       |
| **Qdrant**            | Stores vector embeddings for semantic search |
| **Config Manager**    | Central config for category, limits, model   |
| **User Interface**    | Web UI for interaction with graphs           |
| **Logger**            | Tracks events, errors, and skipped entries   |
| **Docker Compose**    | Brings it all together for local use         |


A local, platform-independent pipeline for processing research papers from arXiv.org.

## Setup Instructions

pdf save directory is set to E:\AI Research\ in "src\utils\download_pdfs.py"

PDF_DIR = r"E:\AI Research"

This project works on both Windows and Ubuntu/Linux environments.

---

### Prerequisites

- Git
- Python 3.9+ (Python 3.12-slim recommended)
- [UV](https://github.com/astral-sh/uv) (for fast Python dependency management)
- Docker and Docker Compose (for containerized deployment)

---

### Installation (Local, Non-Docker)

#### Linux/macOS/WSL:
```bash
# Make the setup script executable
chmod +x scripts/setup_uv.sh

# Run the setup script
./scripts/setup_uv.sh

# Activate the virtual environment
source .venv/bin/activate
```

#### Windows (PowerShell):
```powershell
# Run the setup script
.\scripts\setup_uv.ps1

# Activate the virtual environment
.venv\Scripts\Activate.ps1
```

---
### Running the Pipeline Locally
Not recommended better to run in docker and this option might be removed or unsupported.
```bash
python -m src.pipeline.run_pipeline --config config/default.yaml
```

---
### Dockerized Deployment - Docker Desktop Running
0. Suggested run in venv from scripts above for your OS
1. **Build and start all services:**
   ```bash
   docker compose up -d
   ```
2. **(Optional) Rebuild the app service after code changes:**
   ```bash
   docker compose up -d
   ```
3. **Build the app service wth logs:**
   run with logs
   ```bash
   docker compose up -d
   docker compose logs -f
   ```
   When you want to shutdown docker env, need it up to explore data
   ```bash
   docker compose down
   ```
4. **Access MongoDB, Neo4j, and Qdrant via their exposed ports.**
   ---
   This runs sync-neo4j service for new pdfs inserted from MongoDB or 1st time run.
   ```bash
   docker compose up --build sync-neo4j
   ```
   *issue, sometimes you need to you use
   mongodb://localhost:27017/onfig
   
   default in config/defaults.yaml
   below changes default and runs. Default may not need to be changed.

   mongodb://mongodb:27017/
   ```bash
   $env:MONGO_URI="mongodb://localhost:27017/onfig"
   python src/utils/download_pdfs.py
   ```

5. **Web UI**
   ---
   To run the web UI, use only, not sure why, or restart Web UI docker service
   ```bash
   docker-compose up -d web-ui
   ```
   http://localhost:3000
---
### Configuration
![Image](https://github.com/user-attachments/assets/7d68b38e-b4a1-49d9-acf4-17b74fb05e22)

- Edit `config/default.yaml` to change categories, fetch limits, or database settings.

---

### Notes

- The default Python version for Docker is now `python:3.12-slim`.
- All persistent data (MongoDB, Neo4j, Qdrant) is stored in Docker volumes.
- For development, use the local virtual environment; for production or multi-service orchestration, use Docker Compose. OR ALWAYS use docker.

---

### Troubleshooting

- If you see `ModuleNotFoundError: No module named 'pymongo'`, ensure you have activated your virtual environment and installed dependencies.
- For Docker issues, ensure Docker Desktop is running and you have sufficient permissions.

---
ðŸ“Š Optional Enhancements
These features are supported or planned:

Local web dashboard for visual exploration

PDF section parsing

Citation parsing

Mathatical model extraction

Example notebooks for research

All components are modular and can be swapped or extended via config.

---
ðŸ’¡ Use Cases
Build local AI paper libraries

Graph analysis of research trends

Offline semantic paper search

Prototyping citation or influence mapping tools

Using similarity search to find related papers to use with LLM to create new papers

Overview extraction
---

## API Address 
http://export.arxiv.org/api/query

List used is in config/defaults.yaml these are for reference, there are more categories available. 

---
cs.AI - Artificial Intelligence

cs.GT - Computer Science and Game Theory

cs.CV - Computer Vision and Pattern Recognition

cs.CL - Computation and Language

cs.DS - Data Structures and Algorithms

cs.LO - Logic in Computer Science

cs.LG - Machine Learning

cs.MA - Multiagent Systems

cs.NE - Neural and Evolutionary Computing

cs.NA - Numerical Analysis

stat - Statistics

stat.ML - Machine Learning

stat.TH - Statistics Theory

math.PR - Probability

q-bio.NC - Neurons and Cognition

physics.data-an - Data Analysis, Statistics and Probability

cs.AI, cs.GT, cs.CV, cs.DS, cs.LO, cs.LG, cs.MA, cs.NE
cs.NA, stat, stat.ML, math.PR, q-bio.NC, physics.data-an

---
For more details about project and status, see the `docs/` directory.
