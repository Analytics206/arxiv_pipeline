# ArXiv Pipeline Configuration

# MongoDB settings
mongo:
  connection_string: "mongodb://mongodb:27017/"
  connection_string_local: "mongodb://localhost:27017/config"
  db_name: "arxiv_papers"
  
# Neo4j settings
neo4j:
  url: "bolt://neo4j:7687"
  url_local: "bolt://localhost:7687"
  user: "neo4j"
  password: "password"

# Qdrant settings
qdrant:
  url: "http://qdrant:6333"
  url_local: "http://localhost:6333"
  collection_name: "arxiv_papers"
  vector_size: 768  # For sentence-transformers models
  # Categories to prioritize for vector storage - these will be processed into Qdrant
  process_categories: # used in sync_qdrant.py
    - "cs.AI"
    # - "cs.CV" 
    # - "cs.LG"
  papers_per_category: 3 # Maximum number of papers to process into vectors per category (0 for unlimited)
  default_pdf_path: "X:/AI Research" # Base directory where PDFs are loaded from for vector processing
  gpu_enabled: true # Enable GPU for vector operations
  gpu_device: 1 # Use the second GPU (index 1)
  # Tracking settings for processed PDFs
  tracking:
    enabled: true # Whether to track processed PDFs
    collection_name: "vector_processed_pdfs" # MongoDB collection to store tracking information
    sync_with_qdrant: true # Whether to sync tracking with actual Qdrant contents

# Qdrant Paper Summary Vector settings
paper_summaries:
  enabled: true
  qdrant:
    collection_name: "papers_summary" # Name of the Qdrant collection for paper summaries
    vector_size: 768  # For sentence-transformers models
  # Categories to prioritize for summary vector processing
  process_categories: # used in sync_summary_vectors.py
    - "cs.AI"
    # - "cs.LG"
  papers_per_category: 1000 # Maximum number of papers to process into summary vectors per category (0 for unlimited)
  date_filter:
    enabled: true         # Set to false to disable date filtering
    start_date: "2025-01-01" # Process papers published on or after this date (format: YYYY-MM-DD)
    end_date: "2025-05-20"   # Process papers published on or before this date (format: YYYY-MM-DD)
    sort_by_date: true    # Sort MongoDB query by published date
  # Tracking settings for processed summaries
  tracking:
    enabled: true # Whether to track processed summaries
    collection_name: "vector_processed_summary" # MongoDB collection to store tracking information
    sync_with_qdrant: true # Whether to sync tracking with actual Qdrant contents

# ArXiv API settings 
arxiv:
  categories: # used in sync_mongodb.py
    - "cs.AI"
    # - "cs.CL"
    # - "cs.CV"
    # - "cs.DS"
    # - "cs.GT" 
    - "cs.LG" 
    # - "cs.LO"
    # - "cs.MA"
    # - "cs.NA"
    # - "cs.NE"
    # - "math.PR"
    # - "physics.data-an"
    # - "q-bio.NC"
    # - "stat"
    # - "stat.ML"
    # - "stat.TH"
  max_results: 200 # Number of papers to fetch per category loop, 200 is max but can fail so set to 100
  sort_by: "submittedDate"
  sort_order: "descending"
  max_iterations: 10 # Number of times to loop the pipeline for each category above
  rate_limit_seconds: 3
  start_date: "2023-01-01" # start_date is used in arxiv api call and save pdfs filters
  end_date: "2025-05-20" # end_date is not used in arxiv api call, only have start_date, used when saving pdfs
  max_no_papers: 25 # Maximum number of papers to fetch per category loop (0 for unlimited)
# Embedding model settings
embedding:
  model_name: "all-MiniLM-L6-v2"  # Smaller model to start with
  batch_size: 32

# PDF storage settings
pdf_storage:
  directory: "X:/AI Research" # where papers are downloaded @download_pdfs.py subdirectory is auto created by category
  papers_per_category: 200 # Maximum number of papers to download per category (0 for unlimited)
  # Categories to prioritize for PDF downloads - if empty, all papers in MongoDB will be considered
  process_categories: # used in download_pdfs.py - comment out categories you don't want to download
    - "cs.AI"
    # - "cs.CL"
    # - "cs.CV"
    # - "cs.DS"
    # - "cs.GT"
    # - "cs.LG"
    # - "cs.LO"
    # - "cs.MA"
    # - "cs.NA"
    # - "cs.NE"
    # - "math.PR"
    # - "physics.data-an"
    # - "q-bio.NC"
    # - "stat"
    # - "stat.ML"
    # - "stat.TH"
  # Date filters for downloading PDFs
  download_date_filter:
    enabled: true         # Set to false to disable date filtering
    start_date: "2023-01-01" # Download papers published on or after this date (format: YYYY-MM-DD)
    end_date: "2025-05-20"   # Download papers published on or before this date (format: YYYY-MM-DD)
    sort_by_date: true    # Sort MongoDB query by published date

# BERTopic settings
bertopic:
  # MongoDB connection settings
  mongo:
    connection_string: "mongodb://mongodb:27017/"  # For Docker
    connection_string_local: "mongodb://localhost:27017/"  # For local development
    db_name: "arxiv_papers"
    papers_collection: "papers"  # Source collection
    topics_collection: "paper_topics"  # Target collection for topics
  # Processing settings
  batch_size: 100  # Number of papers to process in each batch
  max_papers: 30000  # Maximum papers to process (0 for unlimited)
  # Category filter
  categories:
    - "cs.AI"
    - "cs.LG"
  # Date filter
  date_filter:
    enabled: true
    start_date: "2023-01-01"  # Process papers published on or after this date
    end_date: "2025-05-20"    # Process papers published on or before this date

# Top2Vec settings
top2vec:
  # MongoDB connection settings
  mongo:
    connection_string: "mongodb://mongodb:27017/"  # For Docker
    connection_string_local: "mongodb://localhost:27017/"  # For local development
    database: "arxiv_papers"
    papers_collection: "papers"  # Source collection
    topics_collection: "paper_top2vec_topics"  # Target collection for topics
  # Model settings
  min_count: 12  # Minimum word count (reduced from 5)
  speed: "learn"  # Speed vs accuracy tradeoff ('learn', 'deep-learn', 'fast-learn')
  workers: 8  # Number of worker threads
  embedding_model: "doc2vec"  # Embedding model to use (doc2vec is the default and always available)
  # Processing settings
  batch_size: 200  # Number of papers to process in each batch
  max_papers: 2000  # Maximum papers to process (0 for unlimited)
  # Category filter
  categories:
    - "cs.AI"
    # - "cs.LG"
  # Date filter
  date_filter:
    enabled: true
    start_date: "2023-01-01"  # Process papers published on or after this date
    end_date: "2025-05-20"    # Process papers published on or before this date

# Logging settings
logging:
  level: "INFO"
  file: "logs/pipeline.log"