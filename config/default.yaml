# ArXiv Pipeline Configuration

# MongoDB settings
mongo:
  connection_string: "mongodb://mongodb:27017/"
  connection_string_local: "mongodb://localhost:27017/config"
  db_name: "arxiv_papers"
  
# Neo4j settings
neo4j:
  url: "bolt://neo4j:7687"
  url_local: "bolt://localhost:7687"
  user: "neo4j"
  password: "password"

# Qdrant settings
qdrant:
  url: "http://qdrant:6333"
  url_local: "http://localhost:6333"
  collection_name: "arxiv_papers"
  vector_size: 768  # For sentence-transformers models
  # Categories to prioritize for vector storage - these will be processed into Qdrant
  process_categories: # used in sync_qdrant.py
    - "cs.AI"
    # - "cs.CV" 
    # - "cs.LG"
  papers_per_category: 3 # Maximum number of papers to process into vectors per category (0 for unlimited)
  default_pdf_path: "X:/AI Research" # Base directory where PDFs are loaded from for vector processing
  gpu_enabled: true # Enable GPU for vector operations
  gpu_device: 1 # Use the second GPU (index 1)
  # Tracking settings for processed PDFs
  tracking:
    enabled: true # Whether to track processed PDFs
    collection_name: "vector_processed_pdfs" # MongoDB collection to store tracking information
    sync_with_qdrant: true # Whether to sync tracking with actual Qdrant contents

# Qdrant Paper Summary Vector settings
paper_summaries:
  enabled: true
  qdrant:
    collection_name: "papers_summary" # Name of the Qdrant collection for paper summaries
    vector_size: 768  # For sentence-transformers models
  # Categories to prioritize for summary vector processing
  process_categories: # used in sync_summary_vectors.py
    - "cs.AI"
    # - "cs.LG"
  papers_per_category: 10 # Maximum number of papers to process into summary vectors per category (0 for unlimited)
  date_filter:
    enabled: true         # Set to false to disable date filtering
    start_date: "2025-01-01" # Process papers published on or after this date (format: YYYY-MM-DD)
    end_date: "2025-05-31"   # Process papers published on or before this date (format: YYYY-MM-DD)
    sort_by_date: true    # Sort MongoDB query by published date
  # Tracking settings for processed summaries
  tracking:
    enabled: true # Whether to track processed summaries
    collection_name: "vector_processed_summary" # MongoDB collection to store tracking information
    sync_with_qdrant: true # Whether to sync tracking with actual Qdrant contents

# ArXiv API settings 
arxiv:
  categories: # used in sync_mongodb.py
    - "cs.AI"
    - "cs.AR"
    - "cs.CC"
    - "cs.CL"
    - "cs.CE"
    - "cs.CR"
    - "cs.CV"
    - "cs.CY"
    - "cs.DB"
    - "cs.DC"
    - "cs.DS"
    - "cs.GR"
    - "cs.GT" 
    - "cs.IR"
    - "cs.IT"
    - "cs.LG" 
    - "cs.LO"
    - "cs.MA"
    - "cs.NE"
    - "cs.RO"
    - "cs.SD"
    - "cs.SE"
    - "cs.SI"
    - "math.AP"
    - "math.PR"
    - "math.ST"
    - "physics.data-an"
    - "q-bio.NC"
    - "stat.AP"
    - "stat.CO"
    - "stat.ME"
    - "stat.ML"
    - "stat.OT"
  max_results: 200 # Number of papers to fetch per category loop, 200 is max but can fail so set to 100
  sort_by: "submittedDate"
  sort_order: "descending"
  max_iterations: 10 # Number of times to loop the pipeline for each category above
  rate_limit_seconds: 5
  start_date: "2022-01-01" # start_date is used in arxiv api call and save pdfs filters
  end_date: "2025-06-30" # end_date is not used in arxiv api call, only have start_date, used when saving pdfs
  max_no_papers: 10 # Maximum number of no paper fetches before breaking loop for category
# Embedding model settings
embedding:
  model_name: "all-MiniLM-L6-v2"  # Smaller model to start with
  batch_size: 200

# 265,979 papers in arxiv_papers.papers
# PDF storage settings
pdf_storage:
  directory: "X:/AI Research" # where papers are downloaded @download_pdfs.py subdirectory is auto created by category
  papers_per_category: 100 # Maximum number of papers to download per category (0 for unlimited)
  # Categories to prioritize for PDF downloads - if empty, all papers in MongoDB will be considered
  process_categories: # used in download_pdfs.py - comment out categories you don't want to download
    - "cs.AI"
    - "cs.AR"
    - "cs.CC"
    - "cs.CL"
    - "cs.CE"
    - "cs.CR"
    - "cs.CV"
    - "cs.CY"
    - "cs.DB"
    - "cs.DC"
    - "cs.DS"
    - "cs.GR"
    - "cs.GT" 
    - "cs.IR"
    - "cs.IT"
    - "cs.LG" 
    - "cs.LO"
    - "cs.MA"
    - "cs.NE"
    - "cs.RO"
    - "cs.SD"
    - "cs.SE"
    - "cs.SI"
    - "math.AP"
    - "math.PR"
    - "math.ST"
    - "physics.data-an"
    - "q-bio.NC"
    - "stat.AP"
    - "stat.CO"
    - "stat.ME"
    - "stat.ML"
    - "stat.OT"
  # Date filters for downloading PDFs
  download_date_filter:
    enabled: true         # Set to false to disable date filtering
    start_date: "2022-01-01" # Download papers published on or after this date (format: YYYY-MM-DD)
    end_date: "2025-06-30"   # Download papers published on or before this date (format: YYYY-MM-DD)
    sort_by_date: true    # Sort MongoDB query by published date

# BERTopic settings
bertopic:
  # MongoDB connection settings
  mongo:
    connection_string: "mongodb://mongodb:27017/"  # For Docker
    connection_string_local: "mongodb://localhost:27017/"  # For local development
    db_name: "arxiv_papers"
    papers_collection: "papers"  # Source collection
    topics_collection: "paper_bertopic_topics"  # Target collection for topics
  # Processing settings
  batch_size: 200  # Number of papers to process in each batch
  max_papers: 100000  # Maximum papers to process (0 for unlimited)
  # Category filter
  categories:
    - "cs.AI"
    # - "cs.LG"
  # Date filter
  date_filter:
    enabled: true
    start_date: "2022-01-01"  # Process papers published on or after this date
    end_date: "2025-06-30"    # Process papers published on or before this date

# Top2Vec settings
top2vec:
  # MongoDB connection settings
  mongo:
    connection_string: "mongodb://mongodb:27017/"  # For Docker
    connection_string_local: "mongodb://localhost:27017/"  # For local development
    database: "arxiv_papers"
    papers_collection: "papers"  # Source collection
    topics_collection: "paper_top2vec_topics"  # Target collection for topics
  # Model settings
  min_count: 12  # Minimum word count (reduced from 5)
  speed: "learn"  # Speed vs accuracy tradeoff ('learn', 'deep-learn', 'fast-learn')
  workers: 8  # Number of worker threads
  embedding_model: "doc2vec"  # Embedding model to use (doc2vec is the default and always available)
  # Processing settings
  batch_size: 200  # Number of papers to process in each batch
  max_papers: 1000000  # Maximum papers to process (0 for unlimited)
  # Category filter
  categories:
    - "cs.AI"
    # - "cs.LG"
  # Date filter
  date_filter:
    enabled: true
    start_date: "2022-01-01"  # Process papers published on or after this date
    end_date: "2025-06-30"    # Process papers published on or before this date

# Kaggle Dataset Downloader settings
kaggle:
  # The official arXiv dataset on Kaggle (https://www.kaggle.com/datasets/Cornell-University/arxiv)
  dataset: "Cornell-University/arxiv"  # Kaggle dataset identifier
  version: "1"  # Dataset version number (usually 1 for most datasets)
  download_path: "X:/kaggle_arxiv"  # Base directory for downloads (use forward slashes)
  credentials:
    path: "secure/kaggle.json"  # Path to kaggle.json (create 'secure' directory if it doesn't exist)
  logging:
    level: "INFO"  # DEBUG, INFO, WARNING, ERROR, CRITICAL
  
  # IMPORTANT: Before using this script, you must:
  # 1. Accept the dataset's terms on Kaggle (visit the dataset page and click "I Understand and Accept")
  # 2. Create a 'secure' directory in your project root and add your kaggle.json file there
  # 3. Ensure you have sufficient disk space (the arXiv dataset is ~8GB)
  # 4. The 'secure' directory is in .gitignore to protect your credentials

# Logging settings
logging:
  level: "INFO"
  file: "logs/pipeline.log"